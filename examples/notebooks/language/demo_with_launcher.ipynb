{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #04D7FD; padding: 20px; text-align: left;\">\n",
    "    <h1 style=\"color: #000000; font-size: 36px; margin: 0;\">Demo: Data Prep Kit</h1>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Welcome to the demo notebook! Inside, you will find an end-to-end sample data pipeline designed for processing language datasets, beginning with a folder of PDF documents and culminating in a working Retrieval-Augmented Generation (RAG) system. This notebook provides the following transforms for processing the data. \n",
    "\n",
    "- [pdf2parquet](#item1)\n",
    "- [Chunk documents](#item2)\n",
    "- [Exact Dedup](#item3)\n",
    "- [Doc_ID generation](#item4)\n",
    "- [Fuzzy Dedup](#item5)\n",
    "- [Language detection](#item6)\n",
    "- [Doc quality](#item7)\n",
    "- [Filtering](#item8)\n",
    "- [Text encoder](#item9)\n",
    "\n",
    "### Getting started\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# We can set input paths here\n",
    "pdf_input_folder = \"input_data\"\n",
    "\n",
    "if not os.path.exists(pdf_input_folder):\n",
    "    print (\"NO INPUT DATA\")\n",
    "    print (\"Please set `pdf_input_folder` variable to path containing data\")\n",
    "\n",
    "# make sure the paths are correct\n",
    "data_base_path = \"output\"\n",
    "\n",
    "parquet_data_output = os.path.join(data_base_path, \"parquet_input\")\n",
    "\n",
    "chunk_out =  os.path.join(data_base_path, \"chunk_out\")\n",
    "ededup_out =  os.path.join(data_base_path, \"ededup_out\")\n",
    "doc_id_out =  os.path.join(data_base_path, \"doc_id_out\")\n",
    "fdedup_out = os.path.join(data_base_path, \"fdedup_out\")\n",
    "lang_out =  os.path.join(data_base_path,\"lang_out\")\n",
    "dq_out = os.path.join(data_base_path,\"dq_out\")\n",
    "\n",
    "filter_out = os.path.join(data_base_path ,\"filter_out\")\n",
    "encoder_out = os.path.join(data_base_path ,\"encoder_out\")\n",
    "\n",
    "# Main repo root\n",
    "from utils import rootdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\"> 1. Convert data to parquet using pdf2parquet [<-](#top)<a class=\"anchor\" id=\"item1\"></a>\n",
    "_pdf_ to _parquet_ </span>\n",
    "\n",
    "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
    "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this stage input folder contains the zip files, each zip file contains a github repo.\n",
    "\n",
    "input_folder = pdf_input_folder\n",
    "output_folder =  parquet_data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:32:29 INFO - Running locally\n",
      "10:32:29 INFO - pdf2parquet parameters are : {'artifacts_path': None, 'contents_type': <pdf2parquet_contents_types.JSON: 'application/json'>, 'do_table_structure': True, 'do_ocr': False}\n",
      "10:32:29 INFO - data factory data_ is using local data access: input_folder - input_data output_folder - test-data/parquet_input\n",
      "10:32:29 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:32:29 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.pdf'], files to checkpoint ['.parquet']\n",
      "10:32:29 INFO - pipeline id pipeline_id\n",
      "10:32:29 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "10:32:29 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1}\n",
      "10:32:29 INFO - actor creation delay 0\n",
      "10:32:29 INFO - job details {'job category': 'preprocessing', 'job name': 'pdf2parquet', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:32:31,109\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:32:33 INFO - orchestrator started at 2024-07-27 10:32:33\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:32:33 INFO - Number of files is 1, source profile {'max_file_size': 5.398899078369141, 'min_file_size': 5.398899078369141, 'total_file_size': 5.398899078369141}\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:32:33 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 33.39723510760814, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:32:33 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'memory': 2147483648, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:32:33 INFO - Completed 0 files (0.0%)  in 2.181529998779297e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m 10:32:35 INFO - Initializing models\n",
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 8665.92it/s]\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m /Users/dol/scratch/dpk-dev/data-prep-kit/examples/notebooks/language/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m /Users/dol/scratch/dpk-dev/data-prep-kit/examples/notebooks/language/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m   warnings.warn(msg)\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m /Users/dol/scratch/dpk-dev/data-prep-kit/examples/notebooks/language/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:33:16 INFO - Completed processing 1 files in 0.7231972336769104 min\n",
      "\u001b[36m(orchestrate pid=16797)\u001b[0m 10:33:16 INFO - done flushing in 0.00045228004455566406 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m /opt/homebrew/Cellar/python@3.10/3.10.14_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTransformFileProcessor pid=16820)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "10:33:26 INFO - Completed execution in 0.9583522955576579 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from pdf2parquet_transform import (\n",
    "    pdf2parquet_contents_type_cli_param,\n",
    "    pdf2parquet_contents_types,\n",
    ")\n",
    "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
    "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
    "\n",
    "from data_processing.utils import GB, ParamsUtils\n",
    "\n",
    "\n",
    "# create parameters\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8, \"memory\": 2 * GB}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "ingest_config = {\n",
    "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    # \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
    "# launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addd5d13",
   "metadata": {},
   "source": [
    "##  <span style=\"color: green\">   2. Doc chunks [<-](#top)<a class=\"anchor\" id=\"item2\"></a> </span>\n",
    "\n",
    "Split the documents in chunks, according to their layout segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86f84e",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81843d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/parquet_input\n",
      "test-data/chunk_out\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = parquet_data_output\n",
    "output_folder = chunk_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35af440",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d34705d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:33:28 INFO - Running locally\n",
      "10:33:28 INFO - doc_json_chunk parameters are : {'content_column_name': 'contents', 'output_chunk_column_name': 'contents', 'output_path_column_name': 'doc_path'}\n",
      "10:33:28 INFO - data factory data_ is using local data access: input_folder - test-data/parquet_input output_folder - test-data/chunk_out\n",
      "10:33:28 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:33:28 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:33:28 INFO - pipeline id pipeline_id\n",
      "10:33:28 INFO - code location None\n",
      "10:33:28 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:33:28 INFO - actor creation delay 0\n",
      "10:33:28 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_json_chunk', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:33:30,045\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:30 INFO - orchestrator started at 2024-07-27 10:33:30\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:30 INFO - Number of files is 1, source profile {'max_file_size': 0.09999656677246094, 'min_file_size': 0.09999656677246094, 'total_file_size': 0.09999656677246094}\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:30 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.844793701544404, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:30 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:30 INFO - Completed 0 files (0.0%)  in 2.555052439371745e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:31 INFO - Completed processing 1 files in 0.010300854841868082 min\n",
      "\u001b[36m(orchestrate pid=17027)\u001b[0m 10:33:31 INFO - done flushing in 0.027120113372802734 sec\n",
      "10:33:41 INFO - Completed execution in 0.21860238711039226 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import doc_json_chunk transform configuration\n",
    "from doc_json_chunk_transform_ray import DocJsonChunkRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocJsonChunkRayTransformConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "##  <span style=\"color: green\">   3. Exact Dedup [<-](#top)<a class=\"anchor\" id=\"item3\"></a> </span>\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58a0e1f6-ff53-40aa-96b1-096ade4bd1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/chunk_out\n",
      "test-data/ededup_out\n"
     ]
    }
   ],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = chunk_out\n",
    "output_folder = ededup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:33:43 INFO - Running locally\n",
      "10:33:43 INFO - exact dedup params are {'hash_cpu': 0.5, 'num_hashes': 2, 'doc_column': 'contents'}\n",
      "10:33:43 INFO - data factory data_ is using local data access: input_folder - test-data/chunk_out output_folder - test-data/ededup_out\n",
      "10:33:43 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:33:43 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:33:43 INFO - pipeline id pipeline_id\n",
      "10:33:43 INFO - code location None\n",
      "10:33:43 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:33:43 INFO - actor creation delay 0\n",
      "10:33:43 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:33:44,691\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - orchestrator started at 2024-07-27 10:33:45\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - Number of files is 1, source profile {'max_file_size': 0.04008960723876953, 'min_file_size': 0.04008960723876953, 'total_file_size': 0.04008960723876953}\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.26534729078412, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - Completed 0 files (0.0%)  in 2.3166338602701823e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - Completed processing 1 files in 0.008430198828379313 min\n",
      "\u001b[36m(orchestrate pid=17099)\u001b[0m 10:33:45 INFO - done flushing in 0.0011980533599853516 sec\n",
      "10:33:55 INFO - Completed execution in 0.21343230406443278 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import ededup transform configuration\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  4. DOC ID generation [<-](#top)<a class=\"anchor\" id=\"item4\"></a> </span>\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is a pre-requisite for fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/ededup_out\n",
      "test-data/doc_id_out\n"
     ]
    }
   ],
   "source": [
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "input_folder = ededup_out\n",
    "output_folder = doc_id_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:33:57 INFO - Running locally\n",
      "10:33:57 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "10:33:57 INFO - data factory data_ is using local data access: input_folder - test-data/ededup_out output_folder - test-data/doc_id_out\n",
      "10:33:57 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:33:57 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:33:57 INFO - pipeline id pipeline_id\n",
      "10:33:57 INFO - code location None\n",
      "10:33:57 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:33:57 INFO - actor creation delay 0\n",
      "10:33:57 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:33:58,835\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:33:59 INFO - orchestrator started at 2024-07-27 10:33:59\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:33:59 INFO - Number of files is 1, source profile {'max_file_size': 0.04008960723876953, 'min_file_size': 0.04008960723876953, 'total_file_size': 0.04008960723876953}\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:33:59 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.41447296179831, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:33:59 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:33:59 INFO - Completed 0 files (0.0%)  in 2.0821889241536456e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:34:00 INFO - Completed processing 1 files in 0.009217751026153565 min\n",
      "\u001b[36m(orchestrate pid=17164)\u001b[0m 10:34:00 INFO - done flushing in 0.001291036605834961 sec\n",
      "10:34:10 INFO - Completed execution in 0.21440748373667398 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## 5. <span style=\"color: green\">  Fuzzy Dedup [<-](#top)<a class=\"anchor\" id=\"item5\"></a> </span>\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/doc_id_out\n",
      "test-data/fdedup_out\n"
     ]
    }
   ],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "input_folder = doc_id_out\n",
    "output_folder = fdedup_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:34:11 INFO - Running locally\n",
      "10:34:11 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "10:34:11 INFO - data factory data_ is using local data access: input_folder - test-data/doc_id_out output_folder - test-data/fdedup_out\n",
      "10:34:11 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:34:11 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:34:11 INFO - pipeline id pipeline_id\n",
      "10:34:11 INFO - code location None\n",
      "10:34:11 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:34:11 INFO - actor creation delay 0\n",
      "10:34:11 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:34:13,219\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - orchestrator started at 2024-07-27 10:34:13\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Number of files is 1, source profile {'max_file_size': 0.04963970184326172, 'min_file_size': 0.04963970184326172, 'total_file_size': 0.04963970184326172}\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.86055908259004, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:13 INFO - Completed 0 files (0.0%)  in 2.2649765014648438e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:14 INFO - Completed processing 1 files in 0.010571451981862386 min\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:14 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:15 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:15 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:16 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:16 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:16 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:16 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:16 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=17256)\u001b[0m 10:34:16 INFO - processing buckets 0 long, 1175 short\n",
      "\u001b[36m(BucketsHash pid=17256)\u001b[0m 10:34:16 INFO - Done submitting long buckets\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:17 INFO - Done processing buckets in 0.007730897267659505 min\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:17 INFO - creating document snapshots\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=17274)\u001b[0m 10:34:17 INFO - Waiting bucket processing completion. Submitted requests 12\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:19 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:19 INFO - Completed 0 files (0.0%)  in 2.578894297281901e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:20 INFO - Completed processing 1 files in 0.026615893840789794 min\n",
      "\u001b[36m(orchestrate pid=17246)\u001b[0m 10:34:20 INFO - done flushing in 0.0030171871185302734 sec\n",
      "10:34:30 INFO - Completed execution in 0.31942368348439537 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d761-62a7-4a12-ad23-b2c268ad8ed2",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  6. Language identification [<-](#top)<a class=\"anchor\" id=\"item6\"></a> </span>\n",
    "\n",
    "This transform identifies the language of the document components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db05e1e-4c62-4367-93ca-b2ddff95e4b4",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8ec4fb6-fa62-45d1-9aa1-596d7182b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = fdedup_out\n",
    "output_folder = lang_out \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e7e5a-064f-4dca-a017-4211f7a3e980",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48dbb2a3-a6f4-4a3d-bb2f-8491fd063611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:34:32 INFO - Running locally\n",
      "10:34:32 INFO - lang_id parameters are : {'model_credential': 'None', 'model_kind': 'fasttext', 'model_url': 'facebook/fasttext-language-identification', 'content_column_name': 'contents', 'output_lang_column_name': 'lang', 'output_score_column_name': 'score'}\n",
      "10:34:32 INFO - data factory data_ is using local data access: input_folder - test-data/fdedup_out output_folder - test-data/lang_out\n",
      "10:34:32 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:34:32 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:34:32 INFO - pipeline id pipeline_id\n",
      "10:34:32 INFO - code location None\n",
      "10:34:32 INFO - number of workers 1 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:34:32 INFO - actor creation delay 0\n",
      "10:34:32 INFO - job details {'job category': 'preprocessing', 'job name': 'lang_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:34:33,931\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:34 INFO - orchestrator started at 2024-07-27 10:34:34\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:34 INFO - Number of files is 1, source profile {'max_file_size': 0.041443824768066406, 'min_file_size': 0.041443824768066406, 'total_file_size': 0.041443824768066406}\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:34 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.826734924688935, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:34 INFO - Number of workers - 1 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:34 INFO - Completed 0 files (0.0%)  in 1.815954844156901e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=17367)\u001b[0m Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:35 INFO - Completed processing 1 files in 0.021671581268310546 min\n",
      "\u001b[36m(orchestrate pid=17358)\u001b[0m 10:34:35 INFO - done flushing in 0.00040602684020996094 sec\n",
      "10:34:45 INFO - Completed execution in 0.2284095803896586 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from lang_id_transform import (\n",
    "    content_column_name_cli_param,\n",
    "    model_credential_cli_param,\n",
    "    model_kind_cli_param,\n",
    "    model_url_cli_param,\n",
    ")\n",
    "from lang_models import KIND_FASTTEXT\n",
    "from lang_id_transform_ray import LangIdentificationRayTransformConfiguration\n",
    "\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langid_config = {\n",
    "    model_credential_cli_param: None, #\"PUT YOUR OWN HUGGINGFACE CREDENTIAL\",\n",
    "    model_kind_cli_param: KIND_FASTTEXT,\n",
    "    model_url_cli_param: \"facebook/fasttext-language-identification\",\n",
    "    # content_column_name_cli_param: \"text\",\n",
    "}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 1,\n",
    "    # language selection specific parameters\n",
    "    **langid_config,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(LangIdentificationRayTransformConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  7. Document Quality [<-](#top)<a class=\"anchor\" id=\"item7\"></a> </span>\n",
    "\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-data/lang_out\n",
      "test-data/dq_out\n"
     ]
    }
   ],
   "source": [
    "input_folder = lang_out\n",
    "output_folder = dq_out\n",
    "\n",
    "print(input_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:34:47 INFO - Running locally\n",
      "10:34:47 INFO - doc_quality parameters are : {'text_lang': 'en', 'doc_content_column': 'contents', 'bad_word_filepath': '/Users/dol/scratch/dpk-dev/data-prep-kit/transforms/language/doc_quality/python/ldnoobw/en', 's3_cred': None, 'docq_data_factory': <data_processing.data_access.data_access_factory.DataAccessFactory object at 0x33272cc10>}\n",
      "10:34:47 INFO - data factory docq_ is using local configuration without input/output path\n",
      "10:34:47 INFO - data factory docq_ max_files -1, n_sample -1\n",
      "10:34:47 INFO - data factory docq_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:34:47 INFO - data factory data_ is using local data access: input_folder - test-data/lang_out output_folder - test-data/dq_out\n",
      "10:34:47 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:34:47 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:34:47 INFO - pipeline id pipeline_id\n",
      "10:34:47 INFO - code location None\n",
      "10:34:47 INFO - number of workers 3 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:34:47 INFO - actor creation delay 0\n",
      "10:34:47 INFO - job details {'job category': 'preprocessing', 'job name': 'docq', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:34:49,019\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:49 INFO - orchestrator started at 2024-07-27 10:34:49\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:49 INFO - Number of files is 1, source profile {'max_file_size': 0.042949676513671875, 'min_file_size': 0.042949676513671875, 'total_file_size': 0.042949676513671875}\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:49 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.3780532842502, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:49 INFO - Number of workers - 3 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:49 INFO - Completed 0 files (0.0%)  in 2.3802121480305988e-06 min. Waiting for completion\n",
      "\u001b[36m(RayTransformFileProcessor pid=17446)\u001b[0m 10:34:49 INFO - Load badwords found locally from /Users/dol/scratch/dpk-dev/data-prep-kit/transforms/language/doc_quality/python/ldnoobw/en\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:50 INFO - Completed processing 1 files in 0.009600981076558431 min\n",
      "\u001b[36m(orchestrate pid=17436)\u001b[0m 10:34:50 INFO - done flushing in 0.0011382102966308594 sec\n",
      "10:35:00 INFO - Completed execution in 0.21429266134897867 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=17444)\u001b[0m 10:34:50 INFO - Load badwords found locally from /Users/dol/scratch/dpk-dev/data-prep-kit/transforms/language/doc_quality/python/ldnoobw/en\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from doc_quality_transform import (\n",
    "    text_lang_cli_param,\n",
    "    doc_content_column_cli_param,\n",
    "    bad_word_filepath_cli_param,\n",
    ")\n",
    "from doc_quality_transform_ray import DocQualityRayTransformConfiguration\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_quality_basedir = os.path.join(rootdir, \"transforms\", \"language\", \"doc_quality\", \"python\")\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    # doc quality configuration\n",
    "    text_lang_cli_param: \"en\",\n",
    "    doc_content_column_cli_param: \"contents\",\n",
    "    bad_word_filepath_cli_param: os.path.join(doc_quality_basedir, \"ldnoobw\", \"en\"),\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(DocQualityRayTransformConfiguration())\n",
    "# launch\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9839-13b4-4d32-89c5-38f59c5a89f0",
   "metadata": {},
   "source": [
    "## 8. <span style=\"color: green\">   Filtering [<-](#top)<a class=\"anchor\" id=\"item8\"></a> </span>\n",
    "\n",
    "Filter out documents that do not meet the quality threshold for each annotation. The thresholds are computed based on a distributional\n",
    "analysis as well as manual inspection of samples maintaining the balance between data quality and data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54d69-8aee-4f0f-b74c-35dc0609270f",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7991811-b19e-43b5-89ac-b24060c0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = dq_out\n",
    "output_folder = filter_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e05c-aeee-4b53-9dd5-8dfa1afc0ece",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61dea2b0-0e54-4912-8620-886e2b8420ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:35:02 INFO - Running locally\n",
      "10:35:02 INFO - data factory data_ is using local data access: input_folder - test-data/dq_out output_folder - test-data/filter_out\n",
      "10:35:02 INFO - data factory data_ max_files -1, n_sample -1\n",
      "10:35:02 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "10:35:02 INFO - pipeline id pipeline_id\n",
      "10:35:02 INFO - code location None\n",
      "10:35:02 INFO - number of workers 5 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "10:35:02 INFO - actor creation delay 0\n",
      "10:35:02 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-07-27 10:35:03,926\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:04 INFO - orchestrator started at 2024-07-27 10:35:04\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:04 INFO - Number of files is 1, source profile {'max_file_size': 0.050690650939941406, 'min_file_size': 0.050690650939941406, 'total_file_size': 0.050690650939941406}\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:04 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 33.96506347693503, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:04 INFO - Number of workers - 5 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:04 INFO - Completed 0 files (0.0%)  in 2.4318695068359373e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:05 INFO - Completed processing 1 files in 0.008527950445810954 min\n",
      "\u001b[36m(orchestrate pid=17510)\u001b[0m 10:35:05 INFO - done flushing in 0.0014939308166503906 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m 10:35:05 WARNING - Exception Binder Error: Referenced column \"total_num_lines\" not found in FROM clause!\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m Candidate bindings: \"input_table.num_tables\"\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m LINE 1: SELECT * FROM input_table WHERE total_num_lines > 10 AND total_num_line...\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m                                         ^ processing file /Users/dol/scratch/dpk-dev/data-prep-kit/examples/notebooks/language/test-data/dq_out/Granite%20Foundation%20Models.parquet: Traceback (most recent call last):\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m   File \"/Users/dol/scratch/dpk-dev/data-prep-kit/data-processing-lib/python/src/data_processing/runtime/transform_file_processor.py\", line 61, in process_file\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m     out_files, stats = self.transform.transform_binary(file_name=f_name, byte_array=filedata)\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m   File \"/Users/dol/scratch/dpk-dev/data-prep-kit/data-processing-lib/python/src/data_processing/transform/table_transform.py\", line 59, in transform_binary\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m     out_tables, stats = self.transform(table=table, file_name=file_name)\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m   File \"/Users/dol/scratch/dpk-dev/data-prep-kit/transforms/universal/filter/python/src/filter_transform.py\", line 100, in transform\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m     filter_table = duckdb.execute(criterion_sql).arrow()\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m duckdb.duckdb.BinderException: Binder Error: Referenced column \"total_num_lines\" not found in FROM clause!\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m Candidate bindings: \"input_table.num_tables\"\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m LINE 1: SELECT * FROM input_table WHERE total_num_lines > 10 AND total_num_line...\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m                                         ^\n",
      "\u001b[36m(RayTransformFileProcessor pid=17525)\u001b[0m \n",
      "10:35:15 INFO - Completed execution in 0.2138486663500468 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "from filter_transform import (\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# TODO\n",
    "# - decide which rules to apply for filtering\n",
    "\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"total_num_lines > 10 AND total_num_lines < 90\",\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "}\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "launcher_params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(launcher_params | filter_params)\n",
    "# Create the longer to launch with the blocklist transform.\n",
    "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## 9. <span style=\"color: green\">  Text encoding [<-](#top)<a class=\"anchor\" id=\"item9\"></a> </span>\n",
    "\n",
    "Encode text for the vector storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = filter_out\n",
    "output_folder = encoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tokenization_transform_r'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtokenization_transform_r\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizationRayConfiguration\n\u001b[1;32m      3\u001b[0m local_conf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_folder,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_folder\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_folder,\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m worker_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_cpus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.8\u001b[39m}\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenization_transform_r'"
     ]
    }
   ],
   "source": [
    "from tokenization_transform_r import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    # \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    # \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    # \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = PythonTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
