{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbF_Zw3KBazf"
   },
   "source": [
    "# **Data Processing of Code data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/IBM/data-prep-kit/blob/tree/dev/examples/notebooks/code/sample-notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-NOkuTxiP7r",
    "outputId": "043f32fc-c476-433e-86b6-d7e9abd4d285"
   },
   "source": [
    "## Notebook shows a pipeline for processing code data. \n",
    "\n",
    "This sample notebook shows how to process hugging face dataset `codeparrot/github-code` with data prep toolkit.\n",
    "\n",
    "The following transformations are applied in order.\n",
    "\n",
    "1. HF2Parquet\n",
    "2. Exact Dedup\n",
    "3. Doc Id\n",
    "4. Fuzzy Dedup \n",
    "5. Prog Lang Select\n",
    "6. Filtering\n",
    "7. Repo Level Grouping\n",
    "8. Tokenization\n",
    "\n",
    "This notebook requires atleast 8 cpus. \n",
    "To run on google colab you need to change the runtime and choose TPUs. This way colab notebook gets a better machine with more number of cpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install data-prep-toolkit and transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data-prep-toolkit-transforms-ray==0.2.1.dev1\n",
      "  Downloading data_prep_toolkit_transforms_ray-0.2.1.dev1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting data-prep-toolkit-transforms==0.2.1.dev1 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached data_prep_toolkit_transforms-0.2.1.dev1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting data-prep-toolkit-ray==0.2.1.dev0 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached data_prep_toolkit_ray-0.2.1.dev0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting parameterized (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tqdm==4.66.3 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tqdm-4.66.3-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting mmh3==4.1.0 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: xxhash==3.4.1 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.4.1)\n",
      "Collecting scipy==1.12.0 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (165 kB)\n",
      "Collecting networkx==3.3 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting colorlog==6.8.2 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting func-timeout==4.3.5 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached func_timeout-4.3.5-py3-none-any.whl\n",
      "Requirement already satisfied: pandas==2.2.2 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.2.2)\n",
      "Collecting emerge-viz==2.0.0 (from data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached emerge_viz-2.0.0-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting data-prep-toolkit==0.2.1.dev0 (from data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached data_prep_toolkit-0.2.1.dev0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting ray==2.24.0 (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached ray-2.24.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting fastapi>=0.110.2 (from data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting pillow>=10.3.0 (from data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting argparse (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting boto3==1.34.69 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached boto3-1.34.69-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting bs4==0.0.2 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting clamd==1.0.2 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached clamd-1.0.2-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting docling==1.1.2 (from docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached docling-1.1.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting duckdb==0.10.1 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached duckdb-0.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (763 bytes)\n",
      "Collecting fasttext==0.9.2 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached fasttext-0.9.2-cp311-cp311-macosx_14_0_arm64.whl\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.21.4 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.24.5)\n",
      "Collecting langcodes==3.3.0 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting numpy==1.26.4 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting pyarrow==16.1.0 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyarrow-16.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2024.1)\n",
      "Collecting quackling==0.1.0 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached quackling-0.1.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting sentence-transformers==3.0.1 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers==4.38.2 (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2024.1)\n",
      "Collecting wheel (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached wheel-0.44.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting autopep8 (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached autopep8-2.3.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting coloredlogs (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting interrogate (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached interrogate-1.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting scikit-learn (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached scikit_learn-1.5.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting prettytable (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached prettytable-3.11.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting py (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pycodestyle (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pycodestyle-2.12.1-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting pylint (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pylint-3.2.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyparsing (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-louvain (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached python_louvain-0.16-py3-none-any.whl\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.11/site-packages (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (6.0.2)\n",
      "Collecting tabulate (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting PyDriller (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached PyDriller-2.6-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyperclip (from emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyperclip-1.9.0-py3-none-any.whl\n",
      "Collecting botocore<1.35.0,>=1.34.69 (from boto3==1.34.69->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached botocore-1.34.160-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3==1.34.69->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3==1.34.69->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in ./venv/lib/python3.11/site-packages (from bs4==0.0.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.12.3)\n",
      "Collecting deepsearch-glm<1,>=0.19.0 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached deepsearch_glm-0.19.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (10 kB)\n",
      "Collecting docling-core<2.0.0,>=1.1.2 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached docling_core-1.1.2-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting docling-ibm-models<2.0.0,>=1.1.0 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached docling_ibm_models-1.1.1-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.3.0 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pydantic_settings-2.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting pypdfium2<5.0.0,>=4.30.0 (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in ./venv/lib/python3.11/site-packages (from docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.32.3)\n",
      "Collecting easyocr<2.0,>=1.7 (from docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached easyocr-1.7.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pybind11>=2.2 (from fasttext==0.9.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pybind11-2.13.3-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./venv/lib/python3.11/site-packages (from fasttext==0.9.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (68.2.2)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.58 (from quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached llama_index_core-0.10.65-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting click>=7.0 (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.15.4)\n",
      "Requirement already satisfied: jsonschema in ./venv/lib/python3.11/site-packages (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.23.0)\n",
      "Collecting msgpack<2.0.0,>=1.0.0 (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached msgpack-1.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (24.1)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3 (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: aiosignal in ./venv/lib/python3.11/site-packages (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in ./venv/lib/python3.11/site-packages (from ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.4.1)\n",
      "Requirement already satisfied: aiohttp>=3.7 in ./venv/lib/python3.11/site-packages (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.10.3)\n",
      "Collecting aiohttp-cors (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting colorful (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting py-spy>=0.2.0 (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached py_spy-0.3.14-py2.py3-none-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (16 kB)\n",
      "Collecting opencensus (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in ./venv/lib/python3.11/site-packages (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.20.0)\n",
      "Collecting smart-open (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached virtualenv-20.26.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting grpcio>=1.42.0 (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached grpcio-1.65.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting memray (from ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached memray-1.13.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers==3.0.1->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached torch-2.4.0-cp311-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached safetensors-0.4.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.110.2->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.11/site-packages (from fastapi>=0.110.2->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.21.4->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2024.6.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.16.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: attrs in ./venv/lib/python3.11/site-packages (from interrogate->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (24.2.0)\n",
      "Collecting colorama (from interrogate->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.11/site-packages (from prettytable->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.2.13)\n",
      "Collecting gitpython (from PyDriller->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting types-pytz (from PyDriller->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached types_pytz-2024.1.0.20240417-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting lizard (from PyDriller->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached lizard-1.17.10-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: platformdirs>=2.2.0 in ./venv/lib/python3.11/site-packages (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.2.2)\n",
      "Collecting astroid<=3.3.0-dev0,>=3.2.4 (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached astroid-3.2.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting isort!=5.13.0,<6,>=4.2.5 (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mccabe<0.8,>=0.6 (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting tomlkit>=0.10.1 (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: dill>=0.3.6 in ./venv/lib/python3.11/site-packages (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.3.8)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.7->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.3.5)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.7->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.7->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.9.4)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./venv/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.69->boto3==1.34.69->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.2.2)\n",
      "Collecting deepsearch-toolkit>=0.31.0 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached deepsearch_toolkit-1.0.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting matplotlib<4.0.0,>=3.7.1 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting netwulf<0.2.0,>=0.1.5 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached netwulf-0.1.5-py3-none-any.whl\n",
      "Collecting numerize<0.13,>=0.12 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached numerize-0.12-py3-none-any.whl\n",
      "Collecting python-dotenv<2.0.0,>=1.0.0 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting textColor<4.0.0,>=3.0.1 (from deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached textcolor-3.1.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting json-schema-for-humans<2.0.0,>=1.0.0 (from docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached json_schema_for_humans-1.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting poetry<2.0.0,>=1.8.3 (from docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached poetry-1.8.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pyproject-toml<0.0.11,>=0.0.10 (from docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyproject_toml-0.0.10-py3-none-any.whl.metadata (642 bytes)\n",
      "Collecting Distance<0.2.0,>=0.1.3 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached Distance-0.1.3-py3-none-any.whl\n",
      "Collecting apted<2.0.0,>=1.0.3 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached apted-1.0.3-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting jsonlines<4.0.0,>=3.1.0 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting lxml<5.0.0,>=4.9.1 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached lxml-4.9.4-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.7 kB)\n",
      "Collecting mean_average_precision<2022.0.0.0,>=2021.4.26.0 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mean_average_precision-2021.4.26.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting onnxruntime<2.0.0,>=1.16.2 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached onnxruntime-1.18.1-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.3 kB)\n",
      "Collecting opencv-python<5.0.0.0,>=4.9.0.80 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting torchvision>=0.17.2 (from docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached torchvision-0.19.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.0 kB)\n",
      "Collecting opencv-python-headless (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting scikit-image (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached scikit_image-0.24.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached python_bidi-0.6.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.6 kB)\n",
      "Collecting Shapely (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached shapely-2.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyclipper-1.3.0.post5-cp311-cp311-macosx_10_9_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema->ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema->ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema->ray==2.24.0->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.20.0)\n",
      "Collecting SQLAlchemy>=1.4.49 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached SQLAlchemy-2.0.32-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: httpx in ./venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.6.0)\n",
      "Collecting nltk>=3.8.2 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached nltk-3.8.2-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting openai>=1.1.0 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached openai-1.40.6-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3.0.0,>=2.0.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic<3.0.0,>=2.0.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pydantic_core-2.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2024.7.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./venv/lib/python3.11/site-packages (from starlette<0.38.0,>=0.37.2->fastapi>=0.110.2->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.4.0)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers==3.0.1->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==3.0.1->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (3.1.4)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached distlib-0.3.8-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.11/site-packages (from beautifulsoup4->bs4==0.0.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->PyDriller->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting rich>=11.2.0 (from memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting textual>=0.41.0 (from memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached textual-0.76.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi>=0.110.2->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.3.1)\n",
      "Collecting platformdirs>=2.2.0 (from pylint->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pluggy<2.0.0,>=1.0.0 (from deepsearch-toolkit>=0.31.0->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting typer<1.0.0,>=0.9.0 (from typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit>=0.31.0->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.35.0,>=1.34.69->boto3==1.34.69->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->PyDriller->emerge-viz==2.0.0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3.0.dev0,>=2.14.1 (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==3.0.1->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.1.5)\n",
      "Requirement already satisfied: Pygments<3.0.0,>=2.10.0 in ./venv/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.18.0)\n",
      "Collecting dataclasses-json (from llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached dataclasses_json-0.5.14-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting htmlmin<0.2.0,>=0.1.12 (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached htmlmin-0.1.12-py3-none-any.whl\n",
      "Collecting markdown2<3.0.0,>=2.4.1 (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached markdown2-2.5.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached fonttools-4.53.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting simplejson>=3.0 (from netwulf<0.2.0,>=0.1.5->deepsearch-glm<1,>=0.19.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached simplejson-3.19.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting flatbuffers (from onnxruntime<2.0.0,>=1.16.2->docling-ibm-models<2.0.0,>=1.1.0->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.14.0)\n",
      "Collecting build<2.0.0,>=1.0.3 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cachecontrol<0.15.0,>=0.14.0 (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached cachecontrol-0.14.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting cleo<3.0.0,>=2.1.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting crashtest<0.5.0,>=0.4.1 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dulwich<0.22.0,>=0.21.2 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached dulwich-0.21.7-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in ./venv/lib/python3.11/site-packages (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.20.0)\n",
      "Collecting installer<0.8.0,>=0.7.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n",
      "Collecting keyring<25.0.0,>=24.0.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached keyring-24.3.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pexpect<5.0.0,>=4.7.0 in ./venv/lib/python3.11/site-packages (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (4.9.0)\n",
      "Collecting pkginfo<2.0,>=1.10 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pkginfo-1.11.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting poetry-core==1.9.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached poetry_core-1.9.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting poetry-plugin-export<2.0.0,>=1.6.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached poetry_plugin_export-1.8.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pyproject-hooks<2.0.0,>=1.0.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting shellingham<2.0,>=1.5 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting trove-classifiers>=2022.5.19 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached trove_classifiers-2024.7.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting xattr<2.0.0,>=1.0.0 (from poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached xattr-1.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting toml (from pyproject-toml<0.0.11,>=0.0.10->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.2.0->memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.58->quackling==0.1.0->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached imageio-2.35.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached tifffile-2024.8.10-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr<2.0,>=1.7->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers==3.0.1->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached rapidfuzz-3.9.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting jaraco.classes (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting importlib-metadata>=4.11.4 (from keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached importlib_metadata-8.2.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.2.0->memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting mdit-py-plugins (from markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached mdit_py_plugins-0.4.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.11/site-packages (from pexpect<5.0.0,>=4.7.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (0.7.0)\n",
      "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: cffi>=1.16.0 in ./venv/lib/python3.11/site-packages (from xattr<2.0.0,>=1.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (1.17.0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.11/site-packages (from cffi>=1.16.0->xattr<2.0.0,>=1.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1) (2.22)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=4.11.4->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached zipp-3.20.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]>=2.1.0->textual>=0.41.0->memray->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==2.24.0->data-prep-toolkit-ray==0.2.1.dev0->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting more-itertools (from jaraco.classes->keyring<25.0.0,>=24.0.0->poetry<2.0.0,>=1.8.3->docling-core<2.0.0,>=1.1.2->docling==1.1.2->docling[ocr]==1.1.2->data-prep-toolkit-transforms==0.2.1.dev1->data-prep-toolkit-transforms-ray==0.2.1.dev1)\n",
      "  Using cached more_itertools-10.4.0-py3-none-any.whl.metadata (36 kB)\n",
      "Downloading data_prep_toolkit_transforms_ray-0.2.1.dev1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Using cached data_prep_toolkit_ray-0.2.1.dev0-py3-none-any.whl (17 kB)\n",
      "Using cached data_prep_toolkit_transforms-0.2.1.dev1-py3-none-any.whl (97 kB)\n",
      "Using cached emerge_viz-2.0.0-py3-none-any.whl (2.0 MB)\n",
      "Using cached mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl (31.4 MB)\n",
      "Using cached tqdm-4.66.3-py3-none-any.whl (78 kB)\n",
      "Using cached boto3-1.34.69-py3-none-any.whl (139 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached clamd-1.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Using cached data_prep_toolkit-0.2.1.dev0-py3-none-any.whl (63 kB)\n",
      "Using cached docling-1.1.2-py3-none-any.whl (34 kB)\n",
      "Using cached duckdb-0.10.1-cp311-cp311-macosx_11_0_arm64.whl (14.3 MB)\n",
      "Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached pyarrow-16.1.0-cp311-cp311-macosx_11_0_arm64.whl (26.0 MB)\n",
      "Using cached quackling-0.1.0-py3-none-any.whl (12 kB)\n",
      "Using cached ray-2.24.0-cp311-cp311-macosx_11_0_arm64.whl (64.5 MB)\n",
      "Using cached sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Using cached fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Using cached autopep8-2.3.1-py2.py3-none-any.whl (45 kB)\n",
      "Using cached pycodestyle-2.12.1-py2.py3-none-any.whl (31 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached interrogate-1.7.0-py3-none-any.whl (46 kB)\n",
      "Using cached prettytable-3.11.0-py3-none-any.whl (28 kB)\n",
      "Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Using cached PyDriller-2.6-py3-none-any.whl (33 kB)\n",
      "Using cached pylint-3.2.6-py3-none-any.whl (519 kB)\n",
      "Using cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached scikit_learn-1.5.1-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached wheel-0.44.0-py3-none-any.whl (67 kB)\n",
      "Using cached astroid-3.2.4-py3-none-any.whl (276 kB)\n",
      "Using cached botocore-1.34.160-py3-none-any.whl (12.5 MB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached deepsearch_glm-0.19.0-cp311-cp311-macosx_14_0_arm64.whl (6.2 MB)\n",
      "Using cached docling_core-1.1.2-py3-none-any.whl (46 kB)\n",
      "Using cached docling_ibm_models-1.1.1-py3-none-any.whl (86 kB)\n",
      "Using cached easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n",
      "Using cached grpcio-1.65.4-cp311-cp311-macosx_10_9_universal2.whl (10.4 MB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached isort-5.13.2-py3-none-any.whl (92 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached llama_index_core-0.10.65-py3-none-any.whl (15.5 MB)\n",
      "Using cached mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Using cached msgpack-1.0.8-cp311-cp311-macosx_11_0_arm64.whl (84 kB)\n",
      "Using cached protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\n",
      "Using cached py_spy-0.3.14-py2.py3-none-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (3.0 MB)\n",
      "Using cached pybind11-2.13.3-py3-none-any.whl (240 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pydantic_settings-2.4.0-py3-none-any.whl (23 kB)\n",
      "Using cached pypdfium2-4.30.0-py3-none-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Using cached regex-2024.7.24-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached safetensors-0.4.4-cp311-cp311-macosx_11_0_arm64.whl (381 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
      "Using cached torch-2.4.0-cp311-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "Using cached virtualenv-20.26.3-py3-none-any.whl (5.7 MB)\n",
      "Using cached aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached lizard-1.17.10-py2.py3-none-any.whl (66 kB)\n",
      "Using cached memray-1.13.4-cp311-cp311-macosx_11_0_arm64.whl (900 kB)\n",
      "Using cached opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Using cached types_pytz-2024.1.0.20240417-py3-none-any.whl (5.2 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached apted-1.0.3-py3-none-any.whl (40 kB)\n",
      "Using cached deepsearch_toolkit-1.0.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached platformdirs-3.11.0-py3-none-any.whl (17 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached distlib-0.3.8-py2.py3-none-any.whl (468 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "Using cached json_schema_for_humans-1.0.2-py3-none-any.whl (284 kB)\n",
      "Using cached dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
      "Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached lxml-4.9.4-cp311-cp311-macosx_11_0_universal2.whl (8.6 MB)\n",
      "Using cached matplotlib-3.9.2-cp311-cp311-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached mean_average_precision-2021.4.26.0-py3-none-any.whl (14 kB)\n",
      "Using cached nltk-3.8.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached onnxruntime-1.18.1-cp311-cp311-macosx_11_0_universal2.whl (15.9 MB)\n",
      "Using cached openai-1.40.6-py3-none-any.whl (361 kB)\n",
      "Using cached opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Using cached poetry-1.8.3-py3-none-any.whl (249 kB)\n",
      "Using cached poetry_core-1.9.0-py3-none-any.whl (309 kB)\n",
      "Using cached pyproject_toml-0.0.10-py3-none-any.whl (6.9 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached SQLAlchemy-2.0.32-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached textcolor-3.1.0-py3-none-any.whl (14 kB)\n",
      "Using cached textual-0.76.0-py3-none-any.whl (567 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "Using cached torchvision-0.19.0-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached ninja-1.11.1.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "Using cached opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "Using cached pyclipper-1.3.0.post5-cp311-cp311-macosx_10_9_universal2.whl (279 kB)\n",
      "Using cached python_bidi-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (250 kB)\n",
      "Using cached scikit_image-0.24.0-cp311-cp311-macosx_12_0_arm64.whl (13.4 MB)\n",
      "Using cached shapely-2.0.5-cp311-cp311-macosx_11_0_arm64.whl (1.3 MB)\n",
      "Using cached sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "Using cached build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached cachecontrol-0.14.0-py3-none-any.whl (22 kB)\n",
      "Using cached cleo-2.1.0-py3-none-any.whl (78 kB)\n",
      "Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (245 kB)\n",
      "Using cached crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached dulwich-0.21.7-cp311-cp311-macosx_11_0_arm64.whl (475 kB)\n",
      "Using cached fonttools-4.53.1-cp311-cp311-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Using cached google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl (271 kB)\n",
      "Using cached imageio-2.35.0-py3-none-any.whl (315 kB)\n",
      "Using cached installer-0.7.0-py3-none-any.whl (453 kB)\n",
      "Using cached jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "Using cached keyring-24.3.1-py3-none-any.whl (38 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl (66 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached markdown2-2.5.0-py2.py3-none-any.whl (47 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached pkginfo-1.11.1-py3-none-any.whl (31 kB)\n",
      "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Using cached poetry_plugin_export-1.8.0-py3-none-any.whl (10 kB)\n",
      "Using cached proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Using cached pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached simplejson-3.19.3-cp311-cp311-macosx_11_0_arm64.whl (74 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached tifffile-2024.8.10-py3-none-any.whl (225 kB)\n",
      "Using cached trove_classifiers-2024.7.2-py3-none-any.whl (13 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached xattr-1.1.0-cp311-cp311-macosx_11_0_arm64.whl (19 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached importlib_metadata-8.2.0-py3-none-any.whl (25 kB)\n",
      "Using cached linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached rapidfuzz-3.9.6-cp311-cp311-macosx_11_0_arm64.whl (1.5 MB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Using cached mdit_py_plugins-0.4.1-py3-none-any.whl (54 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached zipp-3.20.0-py3-none-any.whl (9.4 kB)\n",
      "Using cached more_itertools-10.4.0-py3-none-any.whl (60 kB)\n",
      "Using cached uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Installing collected packages: trove-classifiers, python-bidi, pyperclip, pyclipper, py-spy, opencensus-context, numerize, ninja, mpmath, mmh3, lizard, htmlmin, func-timeout, flatbuffers, filetype, distlib, Distance, dirtyjson, colorful, clamd, argparse, apted, zipp, wrapt, wheel, urllib3, uc-micro-py, types-pytz, tqdm, tomlkit, toml, threadpoolctl, textColor, tenacity, tabulate, sympy, SQLAlchemy, smmap, simplejson, shellingham, safetensors, regex, rapidfuzz, python-dotenv, pyproject-hooks, pypdfium2, pyparsing, pydantic-core, pycodestyle, pybind11, pyasn1, py, protobuf, prettytable, poetry-core, pluggy, platformdirs, pkginfo, pillow, parameterized, numpy, networkx, mypy-extensions, msgpack, more-itertools, mdurl, mccabe, marshmallow, markdown2, lxml, lazy-loader, langcodes, kiwisolver, jsonref, jsonlines, joblib, jmespath, jiter, isort, installer, humanfriendly, grpcio, greenlet, fonttools, duckdb, distro, cycler, crashtest, colorlog, colorama, click, cachetools, astroid, annotated-types, xattr, virtualenv, typing-inspect, torch, tifffile, starlette, smart-open, Shapely, scipy, rsa, python-louvain, pylint, pydantic, pyasn1-modules, pyarrow, proto-plus, opencv-python-headless, opencv-python, nltk, markdown-it-py, linkify-it-py, jaraco.classes, interrogate, importlib-metadata, imageio, googleapis-common-protos, gitdb, fasttext, dulwich, deprecated, contourpy, coloredlogs, cleo, build, bs4, botocore, autopep8, torchvision, tiktoken, scikit-learn, scikit-image, s3transfer, rich, requests-toolbelt, pydantic-settings, openai, onnxruntime, mean_average_precision, mdit-py-plugins, matplotlib, keyring, google-auth, gitpython, fastapi, dataclasses-json, cachecontrol, aiohttp-cors, typer, tokenizers, ray, pyproject-toml, PyDriller, netwulf, llama-index-core, json-schema-for-humans, google-api-core, easyocr, docling-ibm-models, boto3, transformers, textual, opencensus, emerge-viz, data-prep-toolkit, sentence-transformers, memray, data-prep-toolkit-ray, poetry-plugin-export, poetry, docling-core, deepsearch-toolkit, deepsearch-glm, docling, quackling, data-prep-toolkit-transforms, data-prep-toolkit-transforms-ray\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 4.2.2\n",
      "    Uninstalling platformdirs-4.2.2:\n",
      "      Successfully uninstalled platformdirs-4.2.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 17.0.0\n",
      "    Uninstalling pyarrow-17.0.0:\n",
      "      Successfully uninstalled pyarrow-17.0.0\n",
      "Successfully installed Distance-0.1.3 PyDriller-2.6 SQLAlchemy-2.0.32 Shapely-2.0.5 aiohttp-cors-0.7.0 annotated-types-0.7.0 apted-1.0.3 argparse-1.4.0 astroid-3.2.4 autopep8-2.3.1 boto3-1.34.69 botocore-1.34.160 bs4-0.0.2 build-1.2.1 cachecontrol-0.14.0 cachetools-5.4.0 clamd-1.0.2 cleo-2.1.0 click-8.1.7 colorama-0.4.6 coloredlogs-15.0.1 colorful-0.5.6 colorlog-6.8.2 contourpy-1.2.1 crashtest-0.4.1 cycler-0.12.1 data-prep-toolkit-0.2.1.dev0 data-prep-toolkit-ray-0.2.1.dev0 data-prep-toolkit-transforms-0.2.1.dev1 data-prep-toolkit-transforms-ray-0.2.1.dev1 dataclasses-json-0.5.14 deepsearch-glm-0.19.0 deepsearch-toolkit-1.0.0 deprecated-1.2.14 dirtyjson-1.0.8 distlib-0.3.8 distro-1.9.0 docling-1.1.2 docling-core-1.1.2 docling-ibm-models-1.1.1 duckdb-0.10.1 dulwich-0.21.7 easyocr-1.7.1 emerge-viz-2.0.0 fastapi-0.112.0 fasttext-0.9.2 filetype-1.2.0 flatbuffers-24.3.25 fonttools-4.53.1 func-timeout-4.3.5 gitdb-4.0.11 gitpython-3.1.43 google-api-core-2.19.1 google-auth-2.33.0 googleapis-common-protos-1.63.2 greenlet-3.0.3 grpcio-1.65.4 htmlmin-0.1.12 humanfriendly-10.0 imageio-2.35.0 importlib-metadata-8.2.0 installer-0.7.0 interrogate-1.7.0 isort-5.13.2 jaraco.classes-3.4.0 jiter-0.5.0 jmespath-1.0.1 joblib-1.4.2 json-schema-for-humans-1.0.2 jsonlines-3.1.0 jsonref-1.1.0 keyring-24.3.1 kiwisolver-1.4.5 langcodes-3.3.0 lazy-loader-0.4 linkify-it-py-2.0.3 lizard-1.17.10 llama-index-core-0.10.65 lxml-4.9.4 markdown-it-py-3.0.0 markdown2-2.5.0 marshmallow-3.21.3 matplotlib-3.9.2 mccabe-0.7.0 mdit-py-plugins-0.4.1 mdurl-0.1.2 mean_average_precision-2021.4.26.0 memray-1.13.4 mmh3-4.1.0 more-itertools-10.4.0 mpmath-1.3.0 msgpack-1.0.8 mypy-extensions-1.0.0 networkx-3.3 netwulf-0.1.5 ninja-1.11.1.1 nltk-3.8.2 numerize-0.12 numpy-1.26.4 onnxruntime-1.18.1 openai-1.40.6 opencensus-0.11.4 opencensus-context-0.1.3 opencv-python-4.10.0.84 opencv-python-headless-4.10.0.84 parameterized-0.9.0 pillow-10.4.0 pkginfo-1.11.1 platformdirs-3.11.0 pluggy-1.5.0 poetry-1.8.3 poetry-core-1.9.0 poetry-plugin-export-1.8.0 prettytable-3.11.0 proto-plus-1.24.0 protobuf-5.27.3 py-1.11.0 py-spy-0.3.14 pyarrow-16.1.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pybind11-2.13.3 pyclipper-1.3.0.post5 pycodestyle-2.12.1 pydantic-2.8.2 pydantic-core-2.20.1 pydantic-settings-2.4.0 pylint-3.2.6 pyparsing-3.1.2 pypdfium2-4.30.0 pyperclip-1.9.0 pyproject-hooks-1.1.0 pyproject-toml-0.0.10 python-bidi-0.6.0 python-dotenv-1.0.1 python-louvain-0.16 quackling-0.1.0 rapidfuzz-3.9.6 ray-2.24.0 regex-2024.7.24 requests-toolbelt-1.0.0 rich-13.7.1 rsa-4.9 s3transfer-0.10.2 safetensors-0.4.4 scikit-image-0.24.0 scikit-learn-1.5.1 scipy-1.12.0 sentence-transformers-3.0.1 shellingham-1.5.4 simplejson-3.19.3 smart-open-7.0.4 smmap-5.0.1 starlette-0.37.2 sympy-1.13.2 tabulate-0.9.0 tenacity-8.5.0 textColor-3.1.0 textual-0.76.0 threadpoolctl-3.5.0 tifffile-2024.8.10 tiktoken-0.7.0 tokenizers-0.15.2 toml-0.10.2 tomlkit-0.13.2 torch-2.4.0 torchvision-0.19.0 tqdm-4.66.3 transformers-4.38.2 trove-classifiers-2024.7.2 typer-0.12.3 types-pytz-2024.1.0.20240417 typing-inspect-0.9.0 uc-micro-py-1.0.3 urllib3-1.26.19 virtualenv-20.26.3 wheel-0.44.0 wrapt-1.16.0 xattr-1.1.0 zipp-3.20.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.11/site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.11/site-packages (from datasets) (4.66.3)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./venv/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets) (3.10.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./venv/lib/python3.11/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install data-prep-toolkit-transforms-ray==0.2.1.dev1\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VhIsZViaU2i"
   },
   "source": [
    "\n",
    "### Common Ray params for all transforms\n",
    "\n",
    "NOTE: The parameters can be left unchanged for normal use. In case you want fine grained control on parallelization, you can tweak these params.\n",
    "\n",
    "These are the common config paramters used by all transforms. \n",
    "\n",
    "It is possible to parallelize the workloads on different cpus by using parameters: \n",
    "\n",
    "`runtime_num_worker`: number of parallel workers to be used.\n",
    "\n",
    "`num_cpus`: number of cpus to be used per worker.\n",
    "\n",
    "The option `run_locally: True` is used to start a ray cluster for running these transforms. It helps processing files in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J_UbnF9wbj95"
   },
   "outputs": [],
   "source": [
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "#code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "common_config_params = {\n",
    "        # where to run\n",
    "        \"run_locally\": True,\n",
    "        # orchestrator\n",
    "        \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "        \"runtime_num_workers\": 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will do all the processing in `sample_data` folder. Lets create these folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create if not created\n",
    "!mkdir -p sample_data\n",
    "!mkdir -p sample_data/hf_2_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start the Exploration of data processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xliMSdQEEwYx"
   },
   "source": [
    "## 1. **Huggingface datasets to Parquet**\n",
    "\n",
    "This is the first component of this pipeline. It ingests a dataset `codeparrot/github-code` from huggingface and converts it into\n",
    "parquet files for consumption by the next steps in this data processing pipeline.\n",
    "\n",
    "For this demo we are trying to process a few records. The following fields can be updated in case you want to use more data.\n",
    "\n",
    "_total_files_ = 10 <br/>\n",
    "_rows_per_file_ = 10\n",
    "\n",
    "The output of this stage of the pipeline would be written to `sample_data/hf_2_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wit7ic1GauWN",
    "outputId": "cc9ee442-ea65-446c-d495-e5ac83bd5f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_data/hf_2_parquet/data_0.parquet\n",
      "Writing sample_data/hf_2_parquet/data_1.parquet\n",
      "Writing sample_data/hf_2_parquet/data_2.parquet\n",
      "Writing sample_data/hf_2_parquet/data_3.parquet\n",
      "Writing sample_data/hf_2_parquet/data_4.parquet\n",
      "Writing sample_data/hf_2_parquet/data_5.parquet\n",
      "Writing sample_data/hf_2_parquet/data_6.parquet\n",
      "Writing sample_data/hf_2_parquet/data_7.parquet\n",
      "Writing sample_data/hf_2_parquet/data_8.parquet\n",
      "Writing sample_data/hf_2_parquet/data_9.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import uuid\n",
    "from data_processing.utils import TransformUtils\n",
    "from collections import defaultdict\n",
    "\n",
    "DATASET_NAME='codeparrot/github-code'\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, \n",
    "                  streaming=True, \n",
    "                  split=\"train\",\n",
    "                  trust_remote_code=True)\n",
    "\n",
    "def row_mapper(row):\n",
    "    return {\n",
    "            'ext': TransformUtils.get_file_extension(row['path'])[1],\n",
    "            'document_id': str(uuid.uuid4())\n",
    "            }\n",
    "\n",
    "parquet_data_output = \"sample_data/hf_2_parquet\"\n",
    "\n",
    "def hf_dataset_to_parquet(ds, skip, nrows, file_name, mapper=None, renamed_columns=[]):\n",
    "    dst_ = ds.skip(skip).take(nrows)\n",
    "    data_dict = defaultdict(list)\n",
    "\n",
    "    dst = dst_.map(mapper)\n",
    "\n",
    "    for data in dst:\n",
    "        for k, v in data.items():\n",
    "            data_dict[k].append(v)\n",
    "\n",
    "    for old, new in renamed_columns:\n",
    "        data_dict[new] = data_dict[old]\n",
    "        del data_dict[old]\n",
    "\n",
    "    table = pa.Table.from_pydict(data_dict)\n",
    "    pq.write_table(table, file_name)\n",
    "\n",
    "\n",
    "## Create some parquet files from HF data\n",
    "\n",
    "total_files = 10\n",
    "rows_per_file = 10\n",
    "for num in range(total_files):\n",
    "    file_name = os.path.join(\n",
    "        f\"{parquet_data_output}\",\n",
    "        f\"data_{num}.parquet\"\n",
    "    )\n",
    "    print (f\"Writing {file_name}\")\n",
    "    hf_dataset_to_parquet(ds, \n",
    "                          1 * rows_per_file,\n",
    "                          rows_per_file,\n",
    "                          file_name=file_name,\n",
    "                          mapper=row_mapper,\n",
    "                          renamed_columns=[(\"code\", \"contents\"),\n",
    "                                           (\"path\", \"title\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Exact dedup** \n",
    "\n",
    "This step will try to find exact duplicates in the content and remove them. \n",
    "\n",
    "The transform specific params for ededup are:\n",
    " \n",
    " _ededup_hash_cpu_ -  Number of cpus per worker <br/>\n",
    " _ededup_num_hashes_ - Number of workers used to store hashes <br/>\n",
    " _ededup_doc_column_ - Name of column which has to be checked for deduplication <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRUfjHExbd1g",
    "outputId": "39459ec3-491a-4a6d-c80d-8b9bf1333a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:03 INFO - Running locally\n",
      "12:24:03 INFO - exact dedup params are {'doc_column': 'contents', 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "12:24:03 INFO - data factory data_ is using local data access: input_folder - sample_data/hf_2_parquet output_folder - sample_data/ededup_out\n",
      "12:24:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:03 INFO - pipeline id pipeline_id\n",
      "12:24:03 INFO - code location None\n",
      "12:24:03 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:03 INFO - actor creation delay 0\n",
      "12:24:03 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:08,731\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - orchestrator started at 2024-08-14 12:24:09\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Number of files is 10, source profile {'max_file_size': 0.029517173767089844, 'min_file_size': 0.02950763702392578, 'total_file_size': 0.2951526641845703}\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 16.83437042310834, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 1 files in 0.03783274888992309 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 2 files in 0.03785776694615682 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 3 files in 0.03798995018005371 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 4 files in 0.03801209926605224 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 5 files in 0.038075816631317136 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 6 files in 0.03808294932047526 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 7 files in 0.0381393829981486 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 8 files in 0.03815391461054484 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 8 files (80.0%)  in 0.038155682881673175 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:12 INFO - Completed processing 10 files in 0.038234484195709226 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:12 INFO - done flushing in 0.0006730556488037109 sec\n",
      "12:24:22 INFO - Completed execution in 0.30107436577479046 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "input_folder = parquet_data_output # Output of previous stage is used as input.\n",
    "output_folder = \"sample_data/ededup_out\"\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "ededup_params = {\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params | ededup_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "ededup_launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **DOC ID Generation**\n",
    "\n",
    "This step is required for fuzzy deduplication to run. \n",
    "\n",
    "The transform specific params are:\n",
    "\n",
    "_doc_column_ - specifies name of the column containing the document (required for ID generation) <br/>\n",
    "_hash_column_ - specifies name of the column created to hold the string document id, if None, id is not generated <br/>\n",
    "_int_id_column_ - specifies name of the column created to hold the integer document id, if None, id is not generated <br/>\n",
    "\n",
    "At least one of hash_column or int_id_column must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4cYttNlbgf0",
    "outputId": "72790550-fac1-4dba-a332-fb36e4dcf483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:23 INFO - Running locally\n",
      "12:24:23 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "12:24:23 INFO - data factory data_ is using local data access: input_folder - sample_data/ededup_out output_folder - sample_data/docid_out\n",
      "12:24:23 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:23 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:23 INFO - pipeline id pipeline_id\n",
      "12:24:23 INFO - code location None\n",
      "12:24:23 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:23 INFO - actor creation delay 0\n",
      "12:24:23 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:25,295\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - orchestrator started at 2024-08-14 12:24:26\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Number of files is 10, source profile {'max_file_size': 0.021501541137695312, 'min_file_size': 0.0016269683837890625, 'total_file_size': 0.04510021209716797}\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.46223907545209, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 1 files in 0.011678512891133625 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 2 files in 0.011687012513478597 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 3 files in 0.011702593167622883 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 4 files in 0.011713230609893798 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 5 files in 0.01172266403834025 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 6 files in 0.011732947826385499 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 7 files in 0.011741594473520914 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 8 files in 0.011750646432240804 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 8 files (80.0%)  in 0.01175144910812378 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed processing 10 files in 0.011768094698588054 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - done flushing in 0.0005271434783935547 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "12:24:36 INFO - Completed execution in 0.22158671617507936 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=36882)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ededup_out\"\n",
    "output_folder = \"sample_data/docid_out\"\n",
    "\n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_id_params = {\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = doc_id_params | common_config_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Fuzzy Dedup**\n",
    "\n",
    "The fdedup transforms removes documents that are very similar to each other within a set of parquet files.\n",
    "\n",
    "Some important transform specific params are:\n",
    "\n",
    "**columns used**\n",
    "\n",
    "_fdedup_doc_column_ - Column to be used for deduplication <br/>\n",
    "_fdedup_id_column_ - specifies name of the column created to hold the integer document id <br/>\n",
    "_fdedup_cluster_column_ - specifies name of the column which holds the string document id <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b11MMQEheO6q",
    "outputId": "4e6f4d73-4e60-4a28-b3c5-392c8c220111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:43 INFO - Running locally\n",
      "12:24:43 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "12:24:43 INFO - data factory data_ is using local data access: input_folder - sample_data/docid_out output_folder - sample_data/fdedup_out\n",
      "12:24:43 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:43 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:43 INFO - pipeline id pipeline_id\n",
      "12:24:43 INFO - code location None\n",
      "12:24:43 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:43 INFO - actor creation delay 0\n",
      "12:24:43 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:46,801\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - orchestrator started at 2024-08-14 12:24:47\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Number of files is 2, source profile {'max_file_size': 0.022894859313964844, 'min_file_size': 0.011975288391113281, 'total_file_size': 0.034870147705078125}\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.40004272479564, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Completed 0 files (0.0%)  in 4.21603520711263e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:53 INFO - Completed processing 2 files in 0.09755229552586873 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:53 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:54 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:54 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=37043)\u001b[0m 12:24:55 INFO - processing buckets 0 long, 50 short\n",
      "\u001b[36m(BucketsHash pid=37043)\u001b[0m 12:24:55 INFO - Done submitting long buckets\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=37128)\u001b[0m 12:24:56 INFO - Waiting bucket processing completion. Submitted requests 1\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:56 INFO - Done processing buckets in 0.0104233185450236 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:56 INFO - creating document snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:58 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:58 INFO - Completed 0 files (0.0%)  in 5.53131103515625e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:25:08 INFO - Completed processing 2 files in 0.16376852989196777 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:25:08 INFO - done flushing in 0.0033588409423828125 sec\n",
      "12:25:18 INFO - Completed execution in 0.5702923695246379 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/docid_out\"\n",
    "output_folder = \"sample_data/fdedup_out\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "fdedup_params = {\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| fdedup_params\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Programming Language Select**\n",
    "\n",
    "This transform can select the documents for selected languages which can be specified using selected_languages_file.\n",
    "As an example we are using this [file](https://github.com/IBM/data-prep-kit/blob/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt).\n",
    "\n",
    "It is an annotator which adds a new column which can be used to select allowed languages.\n",
    "\n",
    "The important parameters used by this transform are:\n",
    "\n",
    "_lang_allowed_langs_file_key_ - A file with a list of allowed languages. <br/>\n",
    "_lang_lang_column_key_ - The name of column which has programming language. <br/>\n",
    "_lang_output_column_key_ - The name of annotation column. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGaG8NWUAbAu",
    "outputId": "ac40800f-d48a-4e64-c488-da8a16b7f6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:19 INFO - Running locally\n",
      "12:25:19 INFO - data factory proglang_select_ is using local configuration without input/output path\n",
      "12:25:19 INFO - data factory proglang_select_ max_files -1, n_sample -1\n",
      "12:25:19 INFO - data factory proglang_select_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:19 INFO - data factory data_ is using local data access: input_folder - sample_data/fdedup_out output_folder - sample_data/ps_out\n",
      "12:25:19 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:19 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:19 INFO - pipeline id pipeline_id\n",
      "12:25:19 INFO - code location None\n",
      "12:25:19 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:19 INFO - actor creation delay 0\n",
      "12:25:19 INFO - job details {'job category': 'preprocessing', 'job name': 'proglang_select', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:25:21,437\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - orchestrator started at 2024-08-14 12:25:22\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Number of files is 2, source profile {'max_file_size': 0.022310256958007812, 'min_file_size': 0.011380195617675781, 'total_file_size': 0.033690452575683594}\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.480929565615952, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Getting supported languages from file ./allowed-code-languages.txt\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 ERROR - Error reading file ./allowed-code-languages.txt: [Errno 2] No such file or directory: './allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing_ray/runtime/ray/transform_orchestrator.py\", line 83, in orchestrate\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     \"transform_params\": runtime.get_transform_config(\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform_ray.py\", line 70, in get_transform_config\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     lang_list = _get_supported_languages(\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform.py\", line 33, in _get_supported_languages\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     lang_list, _ = data_access.get_file(lang_file)\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 367, in get_file\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     raise e\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 361, in get_file\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     with open(path, \"rb\") as f:\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m          ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: './allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 ERROR - Exception during execution [Errno 2] No such file or directory: './allowed-code-languages.txt': None\n",
      "12:25:32 INFO - Completed execution in 0.21587000290552774 min, execution result 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/fdedup_out\"\n",
    "output_folder = \"sample_data/ps_out\"\n",
    "\n",
    "# download allowed-code-languages.txt\n",
    "!wget https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt\n",
    "selected_languages_file = \"./allowed-code-languages.txt\"\n",
    "\n",
    "from proglang_select_transform_ray import ProgLangSelectRayConfiguration\n",
    "from proglang_select_transform import (\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| langselect_config\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXu_i9jLAo9H"
   },
   "source": [
    "## 6. **Filter**\n",
    "\n",
    "This transform can be used to filter the documents based on conditions we require. Upto this point in the notebook we have used an\n",
    "annotating transform *programming language select*. We can now use this filter to remove the documents annotated by the annotating transform.\n",
    "\n",
    "We can specify filter criteria and also remove columns we added during the course of this pipeline execution.\n",
    "\n",
    "```python\n",
    "\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAl7B58oAyZQ",
    "outputId": "5fc229ef-bb87-4e34-9302-1670b8832d97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:34 INFO - Running locally\n",
      "12:25:34 INFO - data factory data_ is using local data access: input_folder - sample_data/ps_out output_folder - sample_data/filter_out\n",
      "12:25:34 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:34 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:34 INFO - pipeline id pipeline_id\n",
      "12:25:34 INFO - code location None\n",
      "12:25:34 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:34 INFO - actor creation delay 0\n",
      "12:25:34 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:25:36,680\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37332)\u001b[0m 12:25:37 INFO - orchestrator started at 2024-08-14 12:25:37\n",
      "\u001b[36m(orchestrate pid=37332)\u001b[0m 12:25:37 ERROR - No input files to process - exiting\n",
      "12:25:47 INFO - Completed execution in 0.20964287916819255 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ps_out\"\n",
    "output_folder = \"sample_data/filter_out\"\n",
    "\n",
    "\n",
    "from filter_transform import (\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(common_config_params| filter_params)\n",
    "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Repo Level Ordering**\n",
    "\n",
    "Repo Level semantic ordering grouped by language.\n",
    "\n",
    "This transform ouputs one parquet per repo. Additionally it can sort the contents of parquet at repo level using semantic algorithm or by file name. It also has a switch to organise output by programming languages, where dominant language per repo is chosen.\n",
    "It can use local filesystem storage for small data on local node and ray store for scalable  store.\n",
    "\n",
    "This transform has following parameters:\n",
    "\n",
    " _repo_lvl_sorting_enabled_ - If True, the repo level output is sorted using _repo_lvl_sorting_algo_ <br/>\n",
    " _repo_lvl_sorting_algo_ - Sorting algorithm to be used for repo level sorting. It can be (SORT_BY_PATH, SORT_SEMANTIC_NORMALISED, SORT_BY_PATH) <br/>\n",
    " _repo_lvl_store_type_ - Store to build groupby information. Simplest is \"local\" when used locally, \"ray\" when used on cluster <br/>\n",
    " _repo_lvl_store_backend_dir_ -  Directory to use for local store. Needed only when repo_lvl_store_type=local <br/>\n",
    " _repo_lvl_output_by_langs_ - If True, it organises output into folders of programming language. <br/>\n",
    " _repo_lvl_combine_rows_ - If True, it combines the contents of repo into a single row. <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:48 INFO - Running locally\n",
      "12:25:48 INFO - data factory data_ is using local data access: input_folder - sample_data/filter_out output_folder - sample_data/rlo_out\n",
      "12:25:48 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:48 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:48 INFO - pipeline id pipeline_id\n",
      "12:25:48 INFO - code location None\n",
      "12:25:48 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:48 INFO - actor creation delay 0\n",
      "12:25:48 INFO - job details {'job category': 'preprocessing', 'job name': 'repo_lvl', 'job type': 'ray', 'job id': 'job_id'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Store Params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 12:25:50,695\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37352)\u001b[0m 12:25:51 INFO - orchestrator started at 2024-08-14 12:25:51\n",
      "\u001b[36m(orchestrate pid=37352)\u001b[0m 12:25:51 ERROR - No input files to process - exiting\n",
      "12:26:01 INFO - Completed execution in 0.20973085165023803 min, execution result 0\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"sample_data/filter_out\"\n",
    "output_folder = \"sample_data/rlo_out\"\n",
    "\n",
    "import tempfile\n",
    "from repo_level_order_transform import RepoLevelOrderRayTransformConfiguration\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "    # create parameters\n",
    "    local_conf = {\n",
    "        \"input_folder\": input_folder,\n",
    "        \"output_folder\": output_folder,\n",
    "     }\n",
    "\n",
    "    worker_options = {\"num_cpus\": 0.8}\n",
    "    code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "\n",
    "    repo_level_params = {\n",
    "        \"repo_lvl_sorting_algo\": \"SORT_SEMANTIC_NORMALISED\",\n",
    "        \"repo_lvl_store_type\": \"local\",\n",
    "        \"repo_lvl_store_backend_dir\": tmpdirname,\n",
    "        \"repo_lvl_output_by_langs\": True,\n",
    "        \"repo_lvl_combine_rows\": True,\n",
    "        \"repo_lvl_sorting_enabled\": True,\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "    }\n",
    "\n",
    "    \n",
    "    sys.argv = ParamsUtils.dict_to_req(d= common_config_params| repo_level_params)\n",
    "    launcher = RayTransformLauncher(RepoLevelOrderRayTransformConfiguration())\n",
    "    launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byK75Kb1A3E7"
   },
   "source": [
    "## 8. **Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBYg93WMBBq6",
    "outputId": "b3e0541e-4a3d-46f4-8809-ccc8778a53fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:26:03 INFO - Running locally\n",
      "12:26:03 INFO - data factory data_ is using local data access: input_folder - sample_data/rlo_out output_folder - sample_data/tokenize_out\n",
      "12:26:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:26:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:26:03 INFO - pipeline id pipeline_id\n",
      "12:26:03 INFO - code location None\n",
      "12:26:03 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:26:03 INFO - actor creation delay 0\n",
      "12:26:03 INFO - job details {'job category': 'preprocessing', 'job name': 'Tokenization', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:26:05,186\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37377)\u001b[0m 12:26:06 INFO - orchestrator started at 2024-08-14 12:26:06\n",
      "\u001b[36m(orchestrate pid=37377)\u001b[0m 12:26:06 ERROR - No input files to process - exiting\n",
      "12:26:16 INFO - Completed execution in 0.22537057002385458 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/rlo_out\"\n",
    "output_folder = \"sample_data/tokenize_out\"\n",
    "\n",
    "from tokenization_transform_ray import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "tf_params= {\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=common_config_params| tf_params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFUrzzjeBFfJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
