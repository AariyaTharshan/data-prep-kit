{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbF_Zw3KBazf"
   },
   "source": [
    "# **Data Processing of Code data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/IBM/data-prep-kit/blob/tree/dev/examples/notebooks/code/sample-notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-NOkuTxiP7r",
    "outputId": "043f32fc-c476-433e-86b6-d7e9abd4d285"
   },
   "source": [
    "## Notebook shows a pipeline for processing code data. \n",
    "\n",
    "This sample notebook shows how to process hugging face dataset `codeparrot/github-code` with data prep toolkit.\n",
    "\n",
    "The following transformations are applied in order.\n",
    "\n",
    "1. HF2Parquet\n",
    "2. Exact Dedup\n",
    "3. Doc Id\n",
    "4. Fuzzy Dedup \n",
    "5. Prog Lang Select\n",
    "6. Filtering\n",
    "7. Repo Level Grouping\n",
    "8. Tokenization\n",
    "\n",
    "This notebook requires atleast 8 cpus. \n",
    "To run on google colab you need to change the runtime and choose TPUs. This way colab notebook gets a better machine with more number of cpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install data-prep-toolkit and transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install data-prep-toolkit-transforms-ray==0.2.1.dev1\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VhIsZViaU2i"
   },
   "source": [
    "\n",
    "### Common Ray params for all transforms\n",
    "\n",
    "NOTE: The parameters can be left unchanged for normal use. In case you want fine grained control on parallelization, you can tweak these params.\n",
    "\n",
    "These are the common config paramters used by all transforms. \n",
    "\n",
    "It is possible to parallelize the workloads on different cpus by using parameters: \n",
    "\n",
    "`runtime_num_worker`: number of parallel workers to be used.\n",
    "\n",
    "`num_cpus`: number of cpus to be used per worker.\n",
    "\n",
    "The option `run_locally: True` is used to start a ray cluster for running these transforms. It helps processing files in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "J_UbnF9wbj95"
   },
   "outputs": [],
   "source": [
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "#code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "common_config_params = {\n",
    "        # where to run\n",
    "        \"run_locally\": True,\n",
    "        # orchestrator\n",
    "        \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "        \"runtime_num_workers\": 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will do all the processing in `sample_data` folder. Lets create these folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create if not created\n",
    "!mkdir -p sample_data\n",
    "!mkdir -p sample_data/hf_2_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start the Exploration of data processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xliMSdQEEwYx"
   },
   "source": [
    "## 1. **Huggingface datasets to Parquet**\n",
    "\n",
    "This is the first component of this pipeline. It ingests a dataset `codeparrot/github-code` from huggingface and converts it into\n",
    "parquet files for consumption by the next steps in this data processing pipeline.\n",
    "\n",
    "For this demo we are trying to process a few records. The following fields can be updated in case you want to use more data.\n",
    "\n",
    "_total_files_ = 10 <br/>\n",
    "_rows_per_file_ = 10\n",
    "\n",
    "The output of this stage of the pipeline would be written to `sample_data/hf_2_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wit7ic1GauWN",
    "outputId": "cc9ee442-ea65-446c-d495-e5ac83bd5f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_data/hf_2_parquet/data_0.parquet\n",
      "Writing sample_data/hf_2_parquet/data_1.parquet\n",
      "Writing sample_data/hf_2_parquet/data_2.parquet\n",
      "Writing sample_data/hf_2_parquet/data_3.parquet\n",
      "Writing sample_data/hf_2_parquet/data_4.parquet\n",
      "Writing sample_data/hf_2_parquet/data_5.parquet\n",
      "Writing sample_data/hf_2_parquet/data_6.parquet\n",
      "Writing sample_data/hf_2_parquet/data_7.parquet\n",
      "Writing sample_data/hf_2_parquet/data_8.parquet\n",
      "Writing sample_data/hf_2_parquet/data_9.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import uuid\n",
    "from data_processing.utils import TransformUtils\n",
    "from collections import defaultdict\n",
    "\n",
    "DATASET_NAME='codeparrot/github-code'\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, \n",
    "                  streaming=True, \n",
    "                  split=\"train\",\n",
    "                  trust_remote_code=True)\n",
    "\n",
    "def row_mapper(row):\n",
    "    return {\n",
    "            'ext': TransformUtils.get_file_extension(row['path'])[1],\n",
    "            'document_id': str(uuid.uuid4())\n",
    "            }\n",
    "\n",
    "parquet_data_output = \"sample_data/hf_2_parquet\"\n",
    "\n",
    "def hf_dataset_to_parquet(ds, skip, nrows, file_name, mapper=None, renamed_columns=[]):\n",
    "    dst_ = ds.skip(skip).take(nrows)\n",
    "    data_dict = defaultdict(list)\n",
    "\n",
    "    dst = dst_.map(mapper)\n",
    "\n",
    "    for data in dst:\n",
    "        for k, v in data.items():\n",
    "            data_dict[k].append(v)\n",
    "\n",
    "    for old, new in renamed_columns:\n",
    "        data_dict[new] = data_dict[old]\n",
    "        del data_dict[old]\n",
    "\n",
    "    table = pa.Table.from_pydict(data_dict)\n",
    "    pq.write_table(table, file_name)\n",
    "\n",
    "\n",
    "## Create some parquet files from HF data\n",
    "\n",
    "total_files = 10\n",
    "rows_per_file = 10\n",
    "for num in range(total_files):\n",
    "    file_name = os.path.join(\n",
    "        f\"{parquet_data_output}\",\n",
    "        f\"data_{num}.parquet\"\n",
    "    )\n",
    "    print (f\"Writing {file_name}\")\n",
    "    hf_dataset_to_parquet(ds, \n",
    "                          1 * rows_per_file,\n",
    "                          rows_per_file,\n",
    "                          file_name=file_name,\n",
    "                          mapper=row_mapper,\n",
    "                          renamed_columns=[(\"code\", \"contents\"),\n",
    "                                           (\"path\", \"title\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Exact dedup** \n",
    "\n",
    "This step will try to find exact duplicates in the content and remove them. \n",
    "\n",
    "The transform specific params for ededup are:\n",
    " \n",
    " _ededup_hash_cpu_ -  Number of cpus per worker <br/>\n",
    " _ededup_num_hashes_ - Number of workers used to store hashes <br/>\n",
    " _ededup_doc_column_ - Name of column which has to be checked for deduplication <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRUfjHExbd1g",
    "outputId": "39459ec3-491a-4a6d-c80d-8b9bf1333a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:03 INFO - Running locally\n",
      "12:24:03 INFO - exact dedup params are {'doc_column': 'contents', 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "12:24:03 INFO - data factory data_ is using local data access: input_folder - sample_data/hf_2_parquet output_folder - sample_data/ededup_out\n",
      "12:24:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:03 INFO - pipeline id pipeline_id\n",
      "12:24:03 INFO - code location None\n",
      "12:24:03 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:03 INFO - actor creation delay 0\n",
      "12:24:03 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:08,731\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - orchestrator started at 2024-08-14 12:24:09\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Number of files is 10, source profile {'max_file_size': 0.029517173767089844, 'min_file_size': 0.02950763702392578, 'total_file_size': 0.2951526641845703}\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 16.83437042310834, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:09 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 1 files in 0.03783274888992309 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 2 files in 0.03785776694615682 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 3 files in 0.03798995018005371 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 4 files in 0.03801209926605224 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 5 files in 0.038075816631317136 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 6 files in 0.03808294932047526 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 7 files in 0.0381393829981486 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 8 files in 0.03815391461054484 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:11 INFO - Completed 8 files (80.0%)  in 0.038155682881673175 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:12 INFO - Completed processing 10 files in 0.038234484195709226 min\n",
      "\u001b[36m(orchestrate pid=36795)\u001b[0m 12:24:12 INFO - done flushing in 0.0006730556488037109 sec\n",
      "12:24:22 INFO - Completed execution in 0.30107436577479046 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "input_folder = parquet_data_output # Output of previous stage is used as input.\n",
    "output_folder = \"sample_data/ededup_out\"\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "ededup_params = {\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params | ededup_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "ededup_launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **DOC ID Generation**\n",
    "\n",
    "This step is required for fuzzy deduplication to run. \n",
    "\n",
    "The transform specific params are:\n",
    "\n",
    "_doc_column_ - specifies name of the column containing the document (required for ID generation) <br/>\n",
    "_hash_column_ - specifies name of the column created to hold the string document id, if None, id is not generated <br/>\n",
    "_int_id_column_ - specifies name of the column created to hold the integer document id, if None, id is not generated <br/>\n",
    "\n",
    "At least one of hash_column or int_id_column must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4cYttNlbgf0",
    "outputId": "72790550-fac1-4dba-a332-fb36e4dcf483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:23 INFO - Running locally\n",
      "12:24:23 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "12:24:23 INFO - data factory data_ is using local data access: input_folder - sample_data/ededup_out output_folder - sample_data/docid_out\n",
      "12:24:23 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:23 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:23 INFO - pipeline id pipeline_id\n",
      "12:24:23 INFO - code location None\n",
      "12:24:23 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:23 INFO - actor creation delay 0\n",
      "12:24:23 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:25,295\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - orchestrator started at 2024-08-14 12:24:26\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Number of files is 10, source profile {'max_file_size': 0.021501541137695312, 'min_file_size': 0.0016269683837890625, 'total_file_size': 0.04510021209716797}\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.46223907545209, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 1 files in 0.011678512891133625 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 2 files in 0.011687012513478597 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 3 files in 0.011702593167622883 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 4 files in 0.011713230609893798 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 5 files in 0.01172266403834025 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 6 files in 0.011732947826385499 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 7 files in 0.011741594473520914 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 8 files in 0.011750646432240804 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed 8 files (80.0%)  in 0.01175144910812378 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - Completed processing 10 files in 0.011768094698588054 min\n",
      "\u001b[36m(orchestrate pid=36879)\u001b[0m 12:24:26 INFO - done flushing in 0.0005271434783935547 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=36883)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\n",
      "12:24:36 INFO - Completed execution in 0.22158671617507936 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=36882)\u001b[0m 12:24:26 WARNING - table is empty, skipping processing\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ededup_out\"\n",
    "output_folder = \"sample_data/docid_out\"\n",
    "\n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_id_params = {\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = doc_id_params | common_config_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Fuzzy Dedup**\n",
    "\n",
    "The fdedup transforms removes documents that are very similar to each other within a set of parquet files.\n",
    "\n",
    "Some important transform specific params are:\n",
    "\n",
    "**columns used**\n",
    "\n",
    "_fdedup_doc_column_ - Column to be used for deduplication <br/>\n",
    "_fdedup_id_column_ - specifies name of the column created to hold the integer document id <br/>\n",
    "_fdedup_cluster_column_ - specifies name of the column which holds the string document id <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b11MMQEheO6q",
    "outputId": "4e6f4d73-4e60-4a28-b3c5-392c8c220111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:24:43 INFO - Running locally\n",
      "12:24:43 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "12:24:43 INFO - data factory data_ is using local data access: input_folder - sample_data/docid_out output_folder - sample_data/fdedup_out\n",
      "12:24:43 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:24:43 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:24:43 INFO - pipeline id pipeline_id\n",
      "12:24:43 INFO - code location None\n",
      "12:24:43 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:24:43 INFO - actor creation delay 0\n",
      "12:24:43 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:24:46,801\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - orchestrator started at 2024-08-14 12:24:47\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Number of files is 2, source profile {'max_file_size': 0.022894859313964844, 'min_file_size': 0.011975288391113281, 'total_file_size': 0.034870147705078125}\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.40004272479564, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:47 INFO - Completed 0 files (0.0%)  in 4.21603520711263e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:53 INFO - Completed processing 2 files in 0.09755229552586873 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:53 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:54 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:54 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:55 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=37043)\u001b[0m 12:24:55 INFO - processing buckets 0 long, 50 short\n",
      "\u001b[36m(BucketsHash pid=37043)\u001b[0m 12:24:55 INFO - Done submitting long buckets\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=37128)\u001b[0m 12:24:56 INFO - Waiting bucket processing completion. Submitted requests 1\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:56 INFO - Done processing buckets in 0.0104233185450236 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:56 INFO - creating document snapshots\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:58 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:24:58 INFO - Completed 0 files (0.0%)  in 5.53131103515625e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:25:08 INFO - Completed processing 2 files in 0.16376852989196777 min\n",
      "\u001b[36m(orchestrate pid=37025)\u001b[0m 12:25:08 INFO - done flushing in 0.0033588409423828125 sec\n",
      "12:25:18 INFO - Completed execution in 0.5702923695246379 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/docid_out\"\n",
    "output_folder = \"sample_data/fdedup_out\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "fdedup_params = {\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| fdedup_params\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Programming Language Select**\n",
    "\n",
    "This transform can select the documents for selected languages which can be specified using selected_languages_file.\n",
    "As an example we are using this [file](https://github.com/IBM/data-prep-kit/blob/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt).\n",
    "\n",
    "It is an annotator which adds a new column which can be used to select allowed languages.\n",
    "\n",
    "The important parameters used by this transform are:\n",
    "\n",
    "_lang_allowed_langs_file_key_ - A file with a list of allowed languages. <br/>\n",
    "_lang_lang_column_key_ - The name of column which has programming language. <br/>\n",
    "_lang_output_column_key_ - The name of annotation column. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGaG8NWUAbAu",
    "outputId": "ac40800f-d48a-4e64-c488-da8a16b7f6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:19 INFO - Running locally\n",
      "12:25:19 INFO - data factory proglang_select_ is using local configuration without input/output path\n",
      "12:25:19 INFO - data factory proglang_select_ max_files -1, n_sample -1\n",
      "12:25:19 INFO - data factory proglang_select_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:19 INFO - data factory data_ is using local data access: input_folder - sample_data/fdedup_out output_folder - sample_data/ps_out\n",
      "12:25:19 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:19 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:19 INFO - pipeline id pipeline_id\n",
      "12:25:19 INFO - code location None\n",
      "12:25:19 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:19 INFO - actor creation delay 0\n",
      "12:25:19 INFO - job details {'job category': 'preprocessing', 'job name': 'proglang_select', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:25:21,437\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - orchestrator started at 2024-08-14 12:25:22\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Number of files is 2, source profile {'max_file_size': 0.022310256958007812, 'min_file_size': 0.011380195617675781, 'total_file_size': 0.033690452575683594}\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Cluster resources: {'cpus': 12, 'gpus': 0, 'memory': 17.480929565615952, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 INFO - Getting supported languages from file ./allowed-code-languages.txt\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 ERROR - Error reading file ./allowed-code-languages.txt: [Errno 2] No such file or directory: './allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing_ray/runtime/ray/transform_orchestrator.py\", line 83, in orchestrate\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     \"transform_params\": runtime.get_transform_config(\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform_ray.py\", line 70, in get_transform_config\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     lang_list = _get_supported_languages(\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/proglang_select_transform.py\", line 33, in _get_supported_languages\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     lang_list, _ = data_access.get_file(lang_file)\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 367, in get_file\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     raise e\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m   File \"/Users/touma/data-prep-lab/examples/notebooks/code/venv/lib/python3.11/site-packages/data_processing/data_access/data_access_local.py\", line 361, in get_file\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m     with open(path, \"rb\") as f:\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m          ^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: './allowed-code-languages.txt'\n",
      "\u001b[36m(orchestrate pid=37262)\u001b[0m 12:25:22 ERROR - Exception during execution [Errno 2] No such file or directory: './allowed-code-languages.txt': None\n",
      "12:25:32 INFO - Completed execution in 0.21587000290552774 min, execution result 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/fdedup_out\"\n",
    "output_folder = \"sample_data/ps_out\"\n",
    "\n",
    "# download allowed-code-languages.txt\n",
    "!wget https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt\n",
    "selected_languages_file = \"./allowed-code-languages.txt\"\n",
    "\n",
    "from proglang_select_transform_ray import ProgLangSelectRayConfiguration\n",
    "from proglang_select_transform import (\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| langselect_config\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXu_i9jLAo9H"
   },
   "source": [
    "## 6. **Filter**\n",
    "\n",
    "This transform can be used to filter the documents based on conditions we require. Upto this point in the notebook we have used an\n",
    "annotating transform *programming language select*. We can now use this filter to remove the documents annotated by the annotating transform.\n",
    "\n",
    "We can specify filter criteria and also remove columns we added during the course of this pipeline execution.\n",
    "\n",
    "```python\n",
    "\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAl7B58oAyZQ",
    "outputId": "5fc229ef-bb87-4e34-9302-1670b8832d97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:34 INFO - Running locally\n",
      "12:25:34 INFO - data factory data_ is using local data access: input_folder - sample_data/ps_out output_folder - sample_data/filter_out\n",
      "12:25:34 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:34 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:34 INFO - pipeline id pipeline_id\n",
      "12:25:34 INFO - code location None\n",
      "12:25:34 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:34 INFO - actor creation delay 0\n",
      "12:25:34 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:25:36,680\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37332)\u001b[0m 12:25:37 INFO - orchestrator started at 2024-08-14 12:25:37\n",
      "\u001b[36m(orchestrate pid=37332)\u001b[0m 12:25:37 ERROR - No input files to process - exiting\n",
      "12:25:47 INFO - Completed execution in 0.20964287916819255 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ps_out\"\n",
    "output_folder = \"sample_data/filter_out\"\n",
    "\n",
    "\n",
    "from filter_transform import (\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(common_config_params| filter_params)\n",
    "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Repo Level Ordering**\n",
    "\n",
    "Repo Level semantic ordering grouped by language.\n",
    "\n",
    "This transform ouputs one parquet per repo. Additionally it can sort the contents of parquet at repo level using semantic algorithm or by file name. It also has a switch to organise output by programming languages, where dominant language per repo is chosen.\n",
    "It can use local filesystem storage for small data on local node and ray store for scalable  store.\n",
    "\n",
    "This transform has following parameters:\n",
    "\n",
    " _repo_lvl_sorting_enabled_ - If True, the repo level output is sorted using _repo_lvl_sorting_algo_ <br/>\n",
    " _repo_lvl_sorting_algo_ - Sorting algorithm to be used for repo level sorting. It can be (SORT_BY_PATH, SORT_SEMANTIC_NORMALISED, SORT_BY_PATH) <br/>\n",
    " _repo_lvl_store_type_ - Store to build groupby information. Simplest is \"local\" when used locally, \"ray\" when used on cluster <br/>\n",
    " _repo_lvl_store_backend_dir_ -  Directory to use for local store. Needed only when repo_lvl_store_type=local <br/>\n",
    " _repo_lvl_output_by_langs_ - If True, it organises output into folders of programming language. <br/>\n",
    " _repo_lvl_combine_rows_ - If True, it combines the contents of repo into a single row. <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:25:48 INFO - Running locally\n",
      "12:25:48 INFO - data factory data_ is using local data access: input_folder - sample_data/filter_out output_folder - sample_data/rlo_out\n",
      "12:25:48 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:25:48 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:25:48 INFO - pipeline id pipeline_id\n",
      "12:25:48 INFO - code location None\n",
      "12:25:48 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:25:48 INFO - actor creation delay 0\n",
      "12:25:48 INFO - job details {'job category': 'preprocessing', 'job name': 'repo_lvl', 'job type': 'ray', 'job id': 'job_id'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Store Params\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-14 12:25:50,695\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37352)\u001b[0m 12:25:51 INFO - orchestrator started at 2024-08-14 12:25:51\n",
      "\u001b[36m(orchestrate pid=37352)\u001b[0m 12:25:51 ERROR - No input files to process - exiting\n",
      "12:26:01 INFO - Completed execution in 0.20973085165023803 min, execution result 0\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"sample_data/filter_out\"\n",
    "output_folder = \"sample_data/rlo_out\"\n",
    "\n",
    "import tempfile\n",
    "from repo_level_order_transform import RepoLevelOrderRayTransformConfiguration\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "    # create parameters\n",
    "    local_conf = {\n",
    "        \"input_folder\": input_folder,\n",
    "        \"output_folder\": output_folder,\n",
    "     }\n",
    "\n",
    "    worker_options = {\"num_cpus\": 0.8}\n",
    "    code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "\n",
    "    repo_level_params = {\n",
    "        \"repo_lvl_sorting_algo\": \"SORT_SEMANTIC_NORMALISED\",\n",
    "        \"repo_lvl_store_type\": \"local\",\n",
    "        \"repo_lvl_store_backend_dir\": tmpdirname,\n",
    "        \"repo_lvl_output_by_langs\": True,\n",
    "        \"repo_lvl_combine_rows\": True,\n",
    "        \"repo_lvl_sorting_enabled\": True,\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "    }\n",
    "\n",
    "    \n",
    "    sys.argv = ParamsUtils.dict_to_req(d= common_config_params| repo_level_params)\n",
    "    launcher = RayTransformLauncher(RepoLevelOrderRayTransformConfiguration())\n",
    "    launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byK75Kb1A3E7"
   },
   "source": [
    "## 8. **Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBYg93WMBBq6",
    "outputId": "b3e0541e-4a3d-46f4-8809-ccc8778a53fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:26:03 INFO - Running locally\n",
      "12:26:03 INFO - data factory data_ is using local data access: input_folder - sample_data/rlo_out output_folder - sample_data/tokenize_out\n",
      "12:26:03 INFO - data factory data_ max_files -1, n_sample -1\n",
      "12:26:03 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "12:26:03 INFO - pipeline id pipeline_id\n",
      "12:26:03 INFO - code location None\n",
      "12:26:03 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "12:26:03 INFO - actor creation delay 0\n",
      "12:26:03 INFO - job details {'job category': 'preprocessing', 'job name': 'Tokenization', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 12:26:05,186\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=37377)\u001b[0m 12:26:06 INFO - orchestrator started at 2024-08-14 12:26:06\n",
      "\u001b[36m(orchestrate pid=37377)\u001b[0m 12:26:06 ERROR - No input files to process - exiting\n",
      "12:26:16 INFO - Completed execution in 0.22537057002385458 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/rlo_out\"\n",
    "output_folder = \"sample_data/tokenize_out\"\n",
    "\n",
    "from tokenization_transform_ray import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "tf_params= {\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=common_config_params| tf_params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFUrzzjeBFfJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
