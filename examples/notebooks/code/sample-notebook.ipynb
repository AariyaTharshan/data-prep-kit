{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbF_Zw3KBazf"
   },
   "source": [
    "# **Data Processing of Code data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/IBM/data-prep-kit/blob/tree/dev/examples/notebooks/code/sample-notebook.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-NOkuTxiP7r",
    "outputId": "043f32fc-c476-433e-86b6-d7e9abd4d285"
   },
   "source": [
    "## Notebook shows a pipeline for processing code data. \n",
    "\n",
    "This sample notebook shows how to process hugging face dataset `codeparrot/github-code` with data prep toolkit.\n",
    "\n",
    "The following transformations are applied in order.\n",
    "\n",
    "1. HF2Parquet\n",
    "2. Exact Dedup\n",
    "3. Doc Id\n",
    "4. Fuzzy Dedup \n",
    "5. Prog Lang Select\n",
    "6. Filtering\n",
    "7. Repo Level Grouping\n",
    "8. Tokenization\n",
    "\n",
    "This notebook requires atleast 8 cpus. \n",
    "To run on google colab you need to change the runtime and choose TPUs. This way colab notebook gets a better machine with more number of cpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install data-prep-toolkit and transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install data-prep-toolkit-transforms-ray==0.2.1.dev0\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VhIsZViaU2i"
   },
   "source": [
    "\n",
    "### Common Ray params for all transforms\n",
    "\n",
    "NOTE: The parameters can be left unchanged for normal use. In case you want fine grained control on parallelization, you can tweak these params.\n",
    "\n",
    "These are the common config paramters used by all transforms. \n",
    "\n",
    "It is possible to parallelize the workloads on different cpus by using parameters: \n",
    "\n",
    "`runtime_num_worker`: number of parallel workers to be used.\n",
    "\n",
    "`num_cpus`: number of cpus to be used per worker.\n",
    "\n",
    "The option `run_locally: True` is used to start a ray cluster for running these transforms. It helps processing files in parallel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "J_UbnF9wbj95"
   },
   "outputs": [],
   "source": [
    "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "#code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "common_config_params = {\n",
    "        # where to run\n",
    "        \"run_locally\": True,\n",
    "        # orchestrator\n",
    "        \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "        \"runtime_num_workers\": 2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will do all the processing in `sample_data` folder. Lets create these folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create if not created\n",
    "!mkdir -p sample_data\n",
    "!mkdir -p sample_data/hf_2_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Start the Exploration of data processing pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xliMSdQEEwYx"
   },
   "source": [
    "## 1. **Huggingface datasets to Parquet**\n",
    "\n",
    "This is the first component of this pipeline. It ingests a dataset `codeparrot/github-code` from huggingface and converts it into\n",
    "parquet files for consumption by the next steps in this data processing pipeline.\n",
    "\n",
    "For this demo we are trying to process a few records. The following fields can be updated in case you want to use more data.\n",
    "\n",
    "_total_files_ = 10 <br/>\n",
    "_rows_per_file_ = 10\n",
    "\n",
    "The output of this stage of the pipeline would be written to `sample_data/hf_2_parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wit7ic1GauWN",
    "outputId": "cc9ee442-ea65-446c-d495-e5ac83bd5f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sample_data/hf_2_parquet/data_0.parquet\n",
      "Writing sample_data/hf_2_parquet/data_1.parquet\n",
      "Writing sample_data/hf_2_parquet/data_2.parquet\n",
      "Writing sample_data/hf_2_parquet/data_3.parquet\n",
      "Writing sample_data/hf_2_parquet/data_4.parquet\n",
      "Writing sample_data/hf_2_parquet/data_5.parquet\n",
      "Writing sample_data/hf_2_parquet/data_6.parquet\n",
      "Writing sample_data/hf_2_parquet/data_7.parquet\n",
      "Writing sample_data/hf_2_parquet/data_8.parquet\n",
      "Writing sample_data/hf_2_parquet/data_9.parquet\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import uuid\n",
    "from data_processing.utils import TransformUtils\n",
    "from collections import defaultdict\n",
    "\n",
    "DATASET_NAME='codeparrot/github-code'\n",
    "\n",
    "ds = load_dataset(DATASET_NAME, \n",
    "                  streaming=True, \n",
    "                  split=\"train\",\n",
    "                  trust_remote_code=True)\n",
    "\n",
    "def row_mapper(row):\n",
    "    return {\n",
    "            'ext': TransformUtils.get_file_extension(row['path'])[1],\n",
    "            'document_id': str(uuid.uuid4())\n",
    "            }\n",
    "\n",
    "parquet_data_output = \"sample_data/hf_2_parquet\"\n",
    "\n",
    "def hf_dataset_to_parquet(ds, skip, nrows, file_name, mapper=None, renamed_columns=[]):\n",
    "    dst_ = ds.skip(skip).take(nrows)\n",
    "    data_dict = defaultdict(list)\n",
    "\n",
    "    dst = dst_.map(mapper)\n",
    "\n",
    "    for data in dst:\n",
    "        for k, v in data.items():\n",
    "            data_dict[k].append(v)\n",
    "\n",
    "    for old, new in renamed_columns:\n",
    "        data_dict[new] = data_dict[old]\n",
    "        del data_dict[old]\n",
    "\n",
    "    table = pa.Table.from_pydict(data_dict)\n",
    "    pq.write_table(table, file_name)\n",
    "\n",
    "\n",
    "## Create some parquet files from HF data\n",
    "\n",
    "total_files = 10\n",
    "rows_per_file = 10\n",
    "for num in range(total_files):\n",
    "    file_name = os.path.join(\n",
    "        f\"{parquet_data_output}\",\n",
    "        f\"data_{num}.parquet\"\n",
    "    )\n",
    "    print (f\"Writing {file_name}\")\n",
    "    hf_dataset_to_parquet(ds, \n",
    "                          1 * rows_per_file,\n",
    "                          rows_per_file,\n",
    "                          file_name=file_name,\n",
    "                          mapper=row_mapper,\n",
    "                          renamed_columns=[(\"code\", \"contents\"),\n",
    "                                           (\"path\", \"title\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Exact dedup** \n",
    "\n",
    "This step will try to find exact duplicates in the content and remove them. \n",
    "\n",
    "The transform specific params for ededup are:\n",
    " \n",
    " _ededup_hash_cpu_ -  Number of cpus per worker <br/>\n",
    " _ededup_num_hashes_ - Number of workers used to store hashes <br/>\n",
    " _ededup_doc_column_ - Name of column which has to be checked for deduplication <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRUfjHExbd1g",
    "outputId": "39459ec3-491a-4a6d-c80d-8b9bf1333a15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:32:14 INFO - Running locally\n",
      "20:32:14 INFO - exact dedup params are {'doc_column': 'contents', 'hash_cpu': 0.5, 'num_hashes': 2}\n",
      "20:32:14 INFO - data factory data_ is using local data access: input_folder - sample_data/hf_2_parquet output_folder - sample_data/ededup_out\n",
      "20:32:14 INFO - data factory data_ max_files -1, n_sample -1\n",
      "20:32:14 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:32:14 INFO - pipeline id pipeline_id\n",
      "20:32:14 INFO - code location None\n",
      "20:32:14 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "20:32:14 INFO - actor creation delay 0\n",
      "20:32:14 INFO - job details {'job category': 'preprocessing', 'job name': 'ededup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 20:32:18,303\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - orchestrator started at 2024-08-14 20:32:19\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Number of files is 10, source profile {'max_file_size': 0.029517173767089844, 'min_file_size': 0.029506683349609375, 'total_file_size': 0.29514026641845703}\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 33.292274475097656, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 1 files in 0.01203980048497518 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 2 files in 0.012046901384989421 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 3 files in 0.012088934580485025 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 4 files in 0.01209408442179362 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 5 files in 0.012134102980295818 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 6 files in 0.012139701843261718 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 7 files in 0.012182148297627766 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 8 files in 0.012187385559082031 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed 8 files (80.0%)  in 0.012188069025675456 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - Completed processing 10 files in 0.012233320871988933 min\n",
      "\u001b[36m(orchestrate pid=21288)\u001b[0m 20:32:19 INFO - done flushing in 0.00040793418884277344 sec\n",
      "20:32:29 INFO - Completed execution in 0.2542128006617228 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from ededup_transform_ray import EdedupRayTransformConfiguration\n",
    "\n",
    "input_folder = parquet_data_output # Output of previous stage is used as input.\n",
    "output_folder = \"sample_data/ededup_out\"\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "ededup_params = {\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params | ededup_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "ededup_launcher = RayTransformLauncher(EdedupRayTransformConfiguration())\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **DOC ID Generation**\n",
    "\n",
    "This step is required for fuzzy deduplication to run. \n",
    "\n",
    "The transform specific params are:\n",
    "\n",
    "_doc_column_ - specifies name of the column containing the document (required for ID generation) <br/>\n",
    "_hash_column_ - specifies name of the column created to hold the string document id, if None, id is not generated <br/>\n",
    "_int_id_column_ - specifies name of the column created to hold the integer document id, if None, id is not generated <br/>\n",
    "\n",
    "At least one of hash_column or int_id_column must be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4cYttNlbgf0",
    "outputId": "72790550-fac1-4dba-a332-fb36e4dcf483"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:32:31 INFO - Running locally\n",
      "20:32:31 INFO - Doc id parameters are : {'doc_column': 'contents', 'hash_column': 'hash_column', 'int_column': 'int_id_column'}\n",
      "20:32:31 INFO - data factory data_ is using local data access: input_folder - sample_data/ededup_out output_folder - sample_data/docid_out\n",
      "20:32:31 INFO - data factory data_ max_files -1, n_sample -1\n",
      "20:32:31 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:32:31 INFO - pipeline id pipeline_id\n",
      "20:32:31 INFO - code location None\n",
      "20:32:31 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "20:32:31 INFO - actor creation delay 0\n",
      "20:32:31 INFO - job details {'job category': 'preprocessing', 'job name': 'doc_id', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 20:32:34,111\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:34 INFO - orchestrator started at 2024-08-14 20:32:34\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:34 INFO - Number of files is 10, source profile {'max_file_size': 0.016908645629882812, 'min_file_size': 0.001689910888671875, 'total_file_size': 0.04033851623535156}\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:34 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 33.87133636511862, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:34 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 1 files in 0.009744079907735188 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 2 files in 0.009753469626108806 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 3 files in 0.00976391633351644 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 4 files in 0.00977250337600708 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 5 files in 0.009780335426330566 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 6 files in 0.009788715839385986 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 7 files in 0.009795033931732177 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 8 files in 0.009807483355204264 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed 8 files (80.0%)  in 0.009808067480723064 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - Completed processing 10 files in 0.009821001688639324 min\n",
      "\u001b[36m(orchestrate pid=21372)\u001b[0m 20:32:35 INFO - done flushing in 0.0003578662872314453 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=21390)\u001b[0m 20:32:35 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=21390)\u001b[0m 20:32:35 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=21390)\u001b[0m 20:32:35 WARNING - table is empty, skipping processing\n",
      "\u001b[36m(RayTransformFileProcessor pid=21390)\u001b[0m 20:32:35 WARNING - table is empty, skipping processing\n",
      "20:32:45 INFO - Completed execution in 0.23450530370076497 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=21391)\u001b[0m 20:32:35 WARNING - table is empty, skipping processing\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ededup_out\"\n",
    "output_folder = \"sample_data/docid_out\"\n",
    "\n",
    "\n",
    "from doc_id_transform_ray import DocIDRayTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "doc_id_params = {\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = doc_id_params | common_config_params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "launcher = RayTransformLauncher(DocIDRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Fuzzy Dedup**\n",
    "\n",
    "The fdedup transforms removes documents that are very similar to each other within a set of parquet files.\n",
    "\n",
    "Some important transform specific params are:\n",
    "\n",
    "**columns used**\n",
    "\n",
    "_fdedup_doc_column_ - Column to be used for deduplication <br/>\n",
    "_fdedup_id_column_ - specifies name of the column created to hold the integer document id <br/>\n",
    "_fdedup_cluster_column_ - specifies name of the column which holds the string document id <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b11MMQEheO6q",
    "outputId": "4e6f4d73-4e60-4a28-b3c5-392c8c220111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:36:44 INFO - Running locally\n",
      "20:36:44 INFO - fuzzy dedup params are {'doc_column': 'contents', 'id_column': 'int_id_column', 'cluster_column': 'hash_column', 'bucket_cpu': 0.5, 'mhash_cpu': 0.5, 'doc_cpu': 0.5, 'num_doc_actors': 2, 'num_minhash_actors': 1, 'num_bucket_actors': 1, 'num_preprocessors': 2, 'num_permutations': 64, 'threshold': 0.8, 'shingles_size': 5, 'delimiters': ' ', 'snapshot_delay': 1, 'use_bucket_snapshot': False, 'use_doc_snapshot': False, 'random_delay_limit': 10, 'worker_options': {'num_cpus': 0.8}}\n",
      "20:36:44 INFO - data factory data_ is using local data access: input_folder - sample_data/docid_out output_folder - sample_data/fdedup_out\n",
      "20:36:44 INFO - data factory data_ max_files -1, n_sample -1\n",
      "20:36:44 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:36:44 INFO - pipeline id pipeline_id\n",
      "20:36:44 INFO - code location None\n",
      "20:36:44 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "20:36:44 INFO - actor creation delay 0\n",
      "20:36:44 INFO - job details {'job category': 'preprocessing', 'job name': 'fdedup', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 20:36:45,803\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - orchestrator started at 2024-08-14 20:36:46\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Number of files is 2, source profile {'max_file_size': 0.0181884765625, 'min_file_size': 0.011182785034179688, 'total_file_size': 0.029371261596679688}\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.45696258544922, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - starting run from the beginning\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - continuing from the very beginning\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Fuzzy: num buckets 5, bucket length 11\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - created 1 bucket actors\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - created 1 minhash actors\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Table preprocessing uses 2 readers\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - created 2 table processor actors\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:46 INFO - Completed 0 files (0.0%)  in 3.1153361002604166e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:54 INFO - Completed processing 2 files in 0.1280173142751058 min\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:54 INFO - creating minhash snapshots\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:55 INFO - minhash snapshots created\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:55 INFO - creating bucket snapshots\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - bucket snapshots created\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - created 2 document actors\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - created 2 bucket processor actors\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - created bucket processor invoker\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - added invoker to bucket collectors\n",
      "\u001b[36m(BucketsHash pid=22122)\u001b[0m 20:36:56 INFO - processing buckets 0 long, 50 short\n",
      "\u001b[36m(BucketsHash pid=22122)\u001b[0m 20:36:56 INFO - Done submitting long buckets\n",
      "\u001b[36m(BucketsHashProcessorInvoker pid=22154)\u001b[0m 20:36:56 INFO - Waiting bucket processing completion. Submitted requests 1\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - Done processing buckets in 0.00852669874827067 min\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:56 INFO - creating document snapshots\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:58 INFO - document snapshots created\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:36:58 INFO - Completed 0 files (0.0%)  in 4.669030507405599e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:37:06 INFO - Completed processing 2 files in 0.12668618361155193 min\n",
      "\u001b[36m(orchestrate pid=22113)\u001b[0m 20:37:06 INFO - done flushing in 0.002846956253051758 sec\n",
      "20:37:16 INFO - Completed execution in 0.5397263805071513 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/docid_out\"\n",
    "output_folder = \"sample_data/fdedup_out\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "fdedup_params = {\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \",\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| fdedup_params\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "fdedup_launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Programming Language Select**\n",
    "\n",
    "This transform can select the documents for selected languages which can be specified using selected_languages_file.\n",
    "As an example we are using this [file](https://github.com/IBM/data-prep-kit/blob/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt).\n",
    "\n",
    "It is an annotator which adds a new column which can be used to select allowed languages.\n",
    "\n",
    "The important parameters used by this transform are:\n",
    "\n",
    "_lang_allowed_langs_file_key_ - A file with a list of allowed languages. <br/>\n",
    "_lang_lang_column_key_ - The name of column which has programming language. <br/>\n",
    "_lang_output_column_key_ - The name of annotation column. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGaG8NWUAbAu",
    "outputId": "ac40800f-d48a-4e64-c488-da8a16b7f6d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-08-14 20:37:25--  https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15 [text/plain]\n",
      "Saving to: ‘allowed-code-languages.txt.2’\n",
      "\n",
      "allowed-code-langua 100%[===================>]      15  --.-KB/s    in 0s      \n",
      "\n",
      "2024-08-14 20:37:25 (563 KB/s) - ‘allowed-code-languages.txt.2’ saved [15/15]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:37:25 INFO - Running locally\n",
      "20:37:25 INFO - data factory proglang_select_ is using local configuration without input/output path\n",
      "20:37:25 INFO - data factory proglang_select_ max_files -1, n_sample -1\n",
      "20:37:25 INFO - data factory proglang_select_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:37:25 INFO - data factory data_ is using local data access: input_folder - sample_data/fdedup_out output_folder - sample_data/ps_out\n",
      "20:37:25 INFO - data factory data_ max_files -1, n_sample -1\n",
      "20:37:25 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:37:25 INFO - pipeline id pipeline_id\n",
      "20:37:25 INFO - code location None\n",
      "20:37:25 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "20:37:25 INFO - actor creation delay 0\n",
      "20:37:25 INFO - job details {'job category': 'preprocessing', 'job name': 'proglang_select', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 20:37:27,226\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - orchestrator started at 2024-08-14 20:37:27\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Number of files is 2, source profile {'max_file_size': 0.017737388610839844, 'min_file_size': 0.010727882385253906, 'total_file_size': 0.02846527099609375}\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.62237548828125, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Getting supported languages from file ./allowed-code-languages.txt\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Supported languages b'Java\\nC\\nGo\\nABAP\\n'\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:27 INFO - Completed 0 files (0.0%)  in 3.631909688313802e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:28 INFO - Completed processing 2 files in 0.009078633785247803 min\n",
      "\u001b[36m(orchestrate pid=22265)\u001b[0m 20:37:28 INFO - done flushing in 0.0006091594696044922 sec\n",
      "20:37:38 INFO - Completed execution in 0.21556053161621094 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/fdedup_out\"\n",
    "output_folder = \"sample_data/ps_out\"\n",
    "\n",
    "# download allowed-code-languages.txt\n",
    "!wget https://raw.githubusercontent.com/IBM/data-prep-kit/dev/transforms/code/proglang_select/python/test-data/languages/allowed-code-languages.txt\n",
    "selected_languages_file = \"./allowed-code-languages.txt\"\n",
    "\n",
    "from proglang_select_transform_ray import ProgLangSelectRayConfiguration\n",
    "from proglang_select_transform import (\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "params = common_config_params| langselect_config\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(ProgLangSelectRayConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXu_i9jLAo9H"
   },
   "source": [
    "## 6. **Filter**\n",
    "\n",
    "This transform can be used to filter the documents based on conditions we require. Upto this point in the notebook we have used an\n",
    "annotating transform *programming language select*. We can now use this filter to remove the documents annotated by the annotating transform.\n",
    "\n",
    "We can specify filter criteria and also remove columns we added during the course of this pipeline execution.\n",
    "\n",
    "```python\n",
    "\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OAl7B58oAyZQ",
    "outputId": "5fc229ef-bb87-4e34-9302-1670b8832d97"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:37:39 INFO - Running locally\n",
      "20:37:39 INFO - data factory data_ is using local data access: input_folder - sample_data/ps_out output_folder - sample_data/filter_out\n",
      "20:37:39 INFO - data factory data_ max_files -1, n_sample -1\n",
      "20:37:39 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "20:37:39 INFO - pipeline id pipeline_id\n",
      "20:37:39 INFO - code location None\n",
      "20:37:39 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "20:37:39 INFO - actor creation delay 0\n",
      "20:37:39 INFO - job details {'job category': 'preprocessing', 'job name': 'filter', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-14 20:37:41,531\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - orchestrator started at 2024-08-14 20:37:42\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - Number of files is 2, source profile {'max_file_size': 0.01799297332763672, 'min_file_size': 0.010979652404785156, 'total_file_size': 0.028972625732421875}\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 34.37731170654297, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - Completed 0 files (0.0%)  in 3.2623608907063803e-06 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - Completed processing 2 files in 0.009443382422129313 min\n",
      "\u001b[36m(orchestrate pid=22327)\u001b[0m 20:37:42 INFO - done flushing in 0.0005462169647216797 sec\n",
      "20:37:52 INFO - Completed execution in 0.21615846951802573 min, execution result 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/ps_out\"\n",
    "output_folder = \"sample_data/filter_out\"\n",
    "\n",
    "\n",
    "from filter_transform import (\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "from filter_transform_ray import FilterRayTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(common_config_params| filter_params)\n",
    "launcher = RayTransformLauncher(FilterRayTransformConfiguration())\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. **Repo Level Ordering**\n",
    "\n",
    "Repo Level semantic ordering grouped by language.\n",
    "\n",
    "This transform ouputs one parquet per repo. Additionally it can sort the contents of parquet at repo level using semantic algorithm or by file name. It also has a switch to organise output by programming languages, where dominant language per repo is chosen.\n",
    "It can use local filesystem storage for small data on local node and ray store for scalable  store.\n",
    "\n",
    "This transform has following parameters:\n",
    "\n",
    " _repo_lvl_sorting_enabled_ - If True, the repo level output is sorted using _repo_lvl_sorting_algo_ <br/>\n",
    " _repo_lvl_sorting_algo_ - Sorting algorithm to be used for repo level sorting. It can be (SORT_BY_PATH, SORT_SEMANTIC_NORMALISED, SORT_BY_PATH) <br/>\n",
    " _repo_lvl_store_type_ - Store to build groupby information. Simplest is \"local\" when used locally, \"ray\" when used on cluster <br/>\n",
    " _repo_lvl_store_backend_dir_ -  Directory to use for local store. Needed only when repo_lvl_store_type=local <br/>\n",
    " _repo_lvl_output_by_langs_ - If True, it organises output into folders of programming language. <br/>\n",
    " _repo_lvl_combine_rows_ - If True, it combines the contents of repo into a single row. <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"sample_data/filter_out\"\n",
    "output_folder = \"sample_data/rlo_out\"\n",
    "\n",
    "import tempfile\n",
    "from repo_level_order_transform import RepoLevelOrderRayTransformConfiguration\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "\n",
    "    # create parameters\n",
    "    local_conf = {\n",
    "        \"input_folder\": input_folder,\n",
    "        \"output_folder\": output_folder,\n",
    "     }\n",
    "\n",
    "    worker_options = {\"num_cpus\": 0.8}\n",
    "    code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "\n",
    "    repo_level_params = {\n",
    "        \"repo_lvl_sorting_algo\": \"SORT_SEMANTIC_NORMALISED\",\n",
    "        \"repo_lvl_store_type\": \"local\",\n",
    "        \"repo_lvl_store_backend_dir\": tmpdirname,\n",
    "        \"repo_lvl_output_by_langs\": True,\n",
    "        \"repo_lvl_combine_rows\": True,\n",
    "        \"repo_lvl_sorting_enabled\": True,\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "    }\n",
    "\n",
    "    \n",
    "    sys.argv = ParamsUtils.dict_to_req(d= common_config_params| repo_level_params)\n",
    "    launcher = RayTransformLauncher(RepoLevelOrderRayTransformConfiguration())\n",
    "    launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byK75Kb1A3E7"
   },
   "source": [
    "## 8. **Tokenization**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kBYg93WMBBq6",
    "outputId": "b3e0541e-4a3d-46f4-8809-ccc8778a53fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:24:18 INFO - Running locally\n",
      "22:24:18 INFO - data factory data_ is using local data access: input_folder - sample_data/rlo_out output_folder - sample_data/tokenize_out\n",
      "22:24:18 INFO - data factory data_ max_files -1, n_sample -1\n",
      "22:24:18 INFO - data factory data_ Not using data sets, checkpointing False, max files -1, random samples -1, files to use ['.parquet'], files to checkpoint ['.parquet']\n",
      "22:24:18 INFO - pipeline id pipeline_id\n",
      "22:24:18 INFO - code location {'github': 'github', 'commit_hash': '12345', 'path': 'path'}\n",
      "22:24:18 INFO - number of workers 2 worker options {'num_cpus': 0.8, 'max_restarts': -1}\n",
      "22:24:18 INFO - actor creation delay 0\n",
      "22:24:18 INFO - job details {'job category': 'preprocessing', 'job name': 'Tokenization', 'job type': 'ray', 'job id': 'job_id'}\n",
      "2024-08-05 22:24:20,133\tINFO worker.py:1744 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:20 INFO - orchestrator started at 2024-08-05 22:24:20\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:21 INFO - Number of files is 10, source profile {'max_file_size': 0.02017688751220703, 'min_file_size': 0.0035142898559570312, 'total_file_size': 0.0942678451538086}\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:21 INFO - Cluster resources: {'cpus': 16, 'gpus': 0, 'memory': 27.670625305734575, 'object_store': 2.0}\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:21 INFO - Number of workers - 2 with {'num_cpus': 0.8, 'max_restarts': -1} each\n",
      "\u001b[36m(RayTransformFileProcessor pid=69324)\u001b[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 1 files in 0.02559736967086792 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 2 files in 0.0256270170211792 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 3 files in 0.02565788427988688 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 4 files in 0.02570856809616089 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 5 files in 0.025715450445810955 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 6 files in 0.025744632879892985 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 7 files in 0.025754153728485107 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 8 files in 0.02577396631240845 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed 8 files (80.0%)  in 0.025774848461151124 min. Waiting for completion\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - Completed processing 10 files in 0.025802048047383626 min\n",
      "\u001b[36m(orchestrate pid=69309)\u001b[0m 22:24:22 INFO - done flushing in 0.0004413127899169922 sec\n",
      "\u001b[36m(RayTransformFileProcessor pid=69324)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (32945 > 2048). Running this sequence through the model will result in indexing errors\n",
      "22:24:32 INFO - Completed execution in 0.2355100671450297 min, execution result 0\n",
      "\u001b[36m(RayTransformFileProcessor pid=69325)\u001b[0m None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTransformFileProcessor pid=69325)\u001b[0m You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "\u001b[36m(RayTransformFileProcessor pid=69325)\u001b[0m Token indices sequence length is longer than the specified maximum sequence length for this model (8277 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_folder = \"sample_data/rlo_out\"\n",
    "output_folder = \"sample_data/tokenize_out\"\n",
    "\n",
    "from tokenization_transform_ray import TokenizationRayConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "tf_params= {\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf)\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=common_config_params| tf_params)\n",
    "# create launcher\n",
    "launcher = RayTransformLauncher(TokenizationRayConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFUrzzjeBFfJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
